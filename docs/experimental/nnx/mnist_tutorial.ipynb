{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/flax/blob/main/docs/mnist_tutorial.ipynb)\n",
    "[![Open On GitHub](https://img.shields.io/badge/Open-on%20GitHub-blue?logo=GitHub)](https://github.com/google/flax/blob/main/docs/mnist_tutorial.ipynb)\n",
    "\n",
    "# MNIST Tutorial\n",
    "\n",
    "Welcome to NNX! This tutorial will guide you through building and training a simple convolutional \n",
    "neural network (CNN) on the MNIST dataset using the NNX API. NNX is a Python neural network library\n",
    "built upon [JAX](https://github.com/google/jax) and currently offered as an experimental module within \n",
    "[Flax](https://github.com/google/flax)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Install NNX\n",
    "\n",
    "Since NNX is under active development, we recommend using the latest version from the Flax GitHub repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Fix text descriptions in this tutorial\n",
    "!pip install git+https://github.com/google/flax.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Load the MNIST Dataset\n",
    "\n",
    "We'll use TensorFlow Datasets (TFDS) for loading and preparing the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds  # TFDS for MNIST\n",
    "import tensorflow as tf             # TensorFlow operations\n",
    "\n",
    "tf.random.set_seed(0) # set random seed for reproducibility\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "train_ds: tf.data.Dataset = tfds.load('mnist', split='train')\n",
    "test_ds: tf.data.Dataset = tfds.load('mnist', split='test')\n",
    "\n",
    "train_ds = train_ds.map(lambda sample: {\n",
    "  'image': tf.cast(sample['image'],tf.float32) / 255,\n",
    "  'label': sample['label']}) # normalize train set\n",
    "test_ds = test_ds.map(lambda sample: {\n",
    "  'image': tf.cast(sample['image'], tf.float32) / 255,\n",
    "  'label': sample['label']}) # normalize test set\n",
    "\n",
    "train_ds = train_ds.repeat(num_epochs).shuffle(1024) # create shuffled dataset by allocating a buffer size of 1024 to randomly draw elements from\n",
    "train_ds = train_ds.batch(batch_size, drop_remainder=True).prefetch(1) # group into batches of batch_size and skip incomplete batch, prefetch the next sample to improve latency\n",
    "test_ds = test_ds.shuffle(1024) # create shuffled dataset by allocating a buffer size of 1024 to randomly draw elements from\n",
    "test_ds = test_ds.batch(batch_size, drop_remainder=True).prefetch(1) # group into batches of batch_size and skip incomplete batch, prefetch the next sample to improve latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. Define the Network with NNX\n",
    "\n",
    "Create a convolutional neural network with NNX by subclassing `nnx.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.experimental import nnx  # NNX API\n",
    "\n",
    "class CNN(nnx.Module):\n",
    "  \"\"\"A simple CNN model.\"\"\"\n",
    "\n",
    "  def __init__(self, *, rngs: nnx.Rngs):\n",
    "    self.conv1 = nnx.Conv(in_features=1, out_features=32, kernel_size=(3, 3), rngs=rngs)\n",
    "    self.conv2 = nnx.Conv(in_features=32, out_features=64, kernel_size=(3, 3), rngs=rngs)\n",
    "    self.linear1 = nnx.Linear(in_features=3136, out_features=256, rngs=rngs)\n",
    "    self.linear2 = nnx.Linear(in_features=256, out_features=10, rngs=rngs)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = nnx.relu(x)\n",
    "    x = nnx.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = self.conv2(x)\n",
    "    x = nnx.relu(x)\n",
    "    x = nnx.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = x.reshape((x.shape[0], -1))  # flatten\n",
    "    x = self.linear1(x)\n",
    "    x = nnx.relu(x)\n",
    "    x = self.linear2(x)\n",
    "    return x\n",
    "  \n",
    "model = CNN(rngs=nnx.Rngs(0))\n",
    "\n",
    "print(f'model = {model}'[:500] + '\\n...\\n')  # print a part of the model\n",
    "print(f'{model.conv1.kernel.value.shape = }') # inspect the shape of the kernel of the first convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Run model\n",
    "\n",
    "Let's put our model to the test!  We'll perform a forward pass with arbitrary data and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "outputId": "2c580f41-bf5d-40ec-f1cf-ab7f319a84da"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp  # JAX NumPy\n",
    "\n",
    "y = model(jnp.ones((1, 28, 28, 1)))\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 4. Define Metrics\n",
    "\n",
    "To track our model's performance, we'll use the [clu](https://github.com/google/CommonLoopUtils) library. If you haven't already, install it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q clu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Let's create a compound metric using clu.metrics.Collection.  This will include both an Accuracy metric for tracking how well our model classifies images, and an Average metric to monitor the average loss over each training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from clu import metrics\n",
    "# from flax import struct   # Flax pytree dataclasses\n",
    "\n",
    "# @struct.dataclass\n",
    "# class Metrics(metrics.Collection):\n",
    "#   accuracy: metrics.Accuracy\n",
    "#   loss: metrics.Average.from_output('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 5. Create the `TrainState`\n",
    "\n",
    "In Flax, a common practice is to use a dataclass to encapsulate the training state, including the step number, parameters, and optimizer state. The [`flax.training.train_state.TrainState`](https://flax.readthedocs.io/en/latest/flax.training.html#train-state) class is ideal for basic use cases, simplifying the process by allowing you to pass a single argument to functions like `train_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "import dataclasses\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class TrainState(nnx.Module):\n",
    "  optimizer: nnx.optimizer.Optimizer\n",
    "  model: nnx.Module\n",
    "  train_metrics: nnx.metrics.Metric\n",
    "  test_metrics: nnx.metrics.Metric\n",
    "\n",
    "learning_rate = 0.005\n",
    "momentum = 0.9\n",
    "\n",
    "tx = optax.adamw(learning_rate, momentum)\n",
    "train_state = TrainState(\n",
    "  optimizer=nnx.optimizer.Optimizer(model=model, tx=tx),\n",
    "  model=model,\n",
    "  train_metrics=nnx.metrics.MultiMetric(accuracy=nnx.metrics.Accuracy(), loss=nnx.metrics.Average()),\n",
    "  test_metrics=nnx.metrics.MultiMetric(accuracy=nnx.metrics.Accuracy(), loss=nnx.metrics.Average())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Since `TrainState` is a JAX pytree, `Module.split` splits the model into `State` and `GraphDef` pytree objects (representing parameters and the graph definition). A custom `TrainState` type holds the static `GraphDef` and metrics.  We use `optax` to create an optimizer (`adamw`) and initialize the `TrainState`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 6. Training step\n",
    "\n",
    "This function takes the `state` and a data `batch` and does the following:\n",
    "\n",
    "* Reconstructs the model with `static.merge` on the `params`.\n",
    "* Runs the neural network on the input image batch.\n",
    "* Calculates cross-entropy loss using \n",
    "  [optax.softmax_cross_entropy_with_integer_labels()](https://optax.readthedocs.io/en/latest/api.html#optax.softmax_cross_entropy_with_integer_labels). Integer labels eliminate the need for one-hot encoding.\n",
    "* Computes the loss function's gradient with `jax.grad`.\n",
    "* Updates model parameters by applying the gradient pytree to the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state: nnx.State, static: nnx.GraphDef, batch):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  train_state = static.merge(state)\n",
    "  model_state, model_static = train_state.model.split()\n",
    "\n",
    "  def loss_fn(model_state):\n",
    "    model = model_static.merge(model_state)\n",
    "    logits = model(batch['image'])\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits=logits, labels=batch['label']).mean()\n",
    "    return loss, logits\n",
    "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "  (loss, logits), grads = grad_fn(model_state)\n",
    "\n",
    "  train_state.train_metrics.update(values=loss, logits=logits, labels=batch['label'])\n",
    "  train_state.optimizer.apply_gradients(grads=grads)\n",
    "  return train_state.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "The [@jax.jit](https://jax.readthedocs.io/en/latest/jax.html#jax.jit) decorator\n",
    "traces the `train_step` function for just-in-time compilation with \n",
    "[XLA](https://www.tensorflow.org/xla), optimizing performance on \n",
    "hardware accelerators.\n",
    "\n",
    "## 7. Metric Computation\n",
    "\n",
    "Create a separate function to calculate loss and accuracy metrics. Loss is determined using the `optax.softmax_cross_entropy_with_integer_labels` function, and accuracy is computed using `clu.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def compute_test_metrics(*, state: nnx.State, static: nnx.GraphDef, batch):\n",
    "  train_state = static.merge(state)\n",
    "  logits = train_state.model(batch['image'])\n",
    "  loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "      logits=logits, labels=batch['label']).mean()\n",
    "  train_state.test_metrics.update(values=loss, logits=logits, labels=batch['label'])\n",
    "  return train_state.split()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 9. Seed randomness\n",
    "\n",
    "For reproducible dataset shuffling (using `tf.data.Dataset.shuffle`), set the TF random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 10. Train and Evaluate\n",
    "\n",
    "**Dataset Preparation:** create a \"shuffled\" dataset\n",
    "- Repeat the dataset for the desired number of training epochs.\n",
    "- Establish a 1024-sample buffer (holding the dataset's initial 1024 samples).\n",
    "  Randomly draw batches from this buffer.\n",
    "- As samples are drawn, replenish the buffer with subsequent dataset samples.\n",
    "\n",
    "**Training Loop:** Iterate through epochs\n",
    "- Sample batches randomly from the dataset.\n",
    "- Execute an optimization step for each training batch.\n",
    "- Calculate mean training metrics across batches within the epoch.\n",
    "- With updated parameters, compute metrics on the test set.\n",
    "- Log train and test metrics for visualization.\n",
    "\n",
    "After 10 training and testing epochs, your model should reach approximately 99% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "outputId": "258a2c76-2c8f-4a9e-d48b-dde57c342a87"
   },
   "outputs": [],
   "source": [
    "num_steps_per_epoch = train_ds.cardinality().numpy() // num_epochs\n",
    "\n",
    "metrics_history = {\n",
    "  'train_loss': [],\n",
    "  'train_accuracy': [],\n",
    "  'test_loss': [],\n",
    "  'test_accuracy': []\n",
    "}\n",
    "\n",
    "state, static = train_state.split()\n",
    "for step,batch in enumerate(train_ds.as_numpy_iterator()):\n",
    "  # Run optimization steps over training batches and compute batch metrics\n",
    "  state, static = train_step(state, static, batch) # get updated train state (which contains the updated parameters)\n",
    "\n",
    "  if (step+1) % num_steps_per_epoch == 0: # one training epoch has passed\n",
    "    # Compute metrics on the test set after each training epoch\n",
    "    for test_batch in test_ds.as_numpy_iterator():\n",
    "      state, static = compute_test_metrics(state=state, static=static, batch=test_batch)\n",
    "\n",
    "    train_state = static.merge(state)\n",
    "    for metric,value in train_state.train_metrics.compute().items(): # compute metrics\n",
    "      metrics_history[f'train_{metric}'].append(value) # record metrics\n",
    "    for metric,value in train_state.test_metrics.compute().items():\n",
    "      metrics_history[f'test_{metric}'].append(value)\n",
    "\n",
    "    train_state.train_metrics.reset() # reset metrics for next training epoch\n",
    "    train_state.test_metrics.reset()\n",
    "    state, static = train_state.split()\n",
    "\n",
    "    print(f\"train epoch: {(step+1) // num_steps_per_epoch}, \"\n",
    "          f\"loss: {metrics_history['train_loss'][-1]}, \"\n",
    "          f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\")\n",
    "    print(f\"test epoch: {(step+1) // num_steps_per_epoch}, \"\n",
    "          f\"loss: {metrics_history['test_loss'][-1]}, \"\n",
    "          f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 11. Visualize Metrics\n",
    "\n",
    "Use Matplotlib to create plots for loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "outputId": "431a2fcd-44fa-4202-f55a-906555f060ac"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # Visualization\n",
    "\n",
    "# Plot loss and accuracy in subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax1.set_title('Loss')\n",
    "ax2.set_title('Accuracy')\n",
    "for dataset in ('train','test'):\n",
    "  ax1.plot(metrics_history[f'{dataset}_loss'], label=f'{dataset}_loss')\n",
    "  ax2.plot(metrics_history[f'{dataset}_accuracy'], label=f'{dataset}_accuracy')\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## 12. Perform inference on test set\n",
    "\n",
    "Define a jitted inference function, `pred_step`, to generate predictions on the test set using the learned model parameters. This will enable you to visualize test images alongside their predicted labels for a qualitative assessment of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def pred_step(state: nnx.State, static: nnx.GraphDef, batch):\n",
    "  train_state = static.merge(state)\n",
    "  logits = train_state.model(batch['image'])\n",
    "  return logits.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "outputId": "1db5a01c-9d70-4f7d-8c0d-0a3ad8252d3e"
   },
   "outputs": [],
   "source": [
    "test_batch = test_ds.as_numpy_iterator().next()\n",
    "pred = pred_step(state, static, test_batch)\n",
    "\n",
    "fig, axs = plt.subplots(5, 5, figsize=(12, 12))\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "  ax.imshow(test_batch['image'][i, ..., 0], cmap='gray')\n",
    "  ax.set_title(f\"label={pred[i]}\")\n",
    "  ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Congratulations! You made it to the end of the annotated MNIST example."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst",
   "main_language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
