{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca360491",
   "metadata": {},
   "source": [
    "# User Guide on Using FP8\n",
    "\n",
    "JAX supports various FP8 formats, including E4M3 (jnp.float8_e4m3fn) and E5M2\n",
    "(jnp.float8_e5m2). Due to the limited range of FP8 data types, higher-precision\n",
    "data must be scaled to fit within the FP8 representable range, a process known\n",
    "as quantization (Q). Conversely, de-quantization (DQ) rescales the FP8 data back\n",
    "to its original type.\n",
    "\n",
    "Although jnp.dot supports FP8 inputs, certain limitations make it impractical\n",
    "for real-world applications. Alternatively, XLA, our compiler, can recognize\n",
    "patterns like <FP8>->DQ->Dot and subsequently invoke FP8 backends (e.g.,\n",
    "cublasLt for GPUs). FLAX encapsulates such patterns into the\n",
    "nn.fp8_ops.Fp8DotGeneralOp module, allowing users to easily configure it for\n",
    "existing layers (e.g., nn.Dense).\n",
    "\n",
    "This tutorial will walk you through the basics of how to use it.\n",
    "\n",
    "## Setting up our environment\n",
    "\n",
    "Here, we provide the code necessary to set up the environment for our notebook.\n",
    "Additionally, we define a function to check if the XLA-optimized HLO will indeed\n",
    "call an FP8 dot operation under the hood.\n",
    "\n",
    "*Note: This tutorial relies on the XLA-FP8 feature, which is only supported on\n",
    "NVIDIA Hopper GPUs or later.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177b91c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "import jax\n",
    "import re\n",
    "import pprint\n",
    "from jax import random\n",
    "from jax import numpy as jnp\n",
    "from jax._src import test_util as jtu\n",
    "from flax import linen as nn\n",
    "from flax.linen import fp8_ops\n",
    "\n",
    "e4m3 = jnp.float8_e4m3fn\n",
    "e5m2 = jnp.float8_e5m2\n",
    "f32 = jnp.float32\n",
    "E4M3_MAX = jnp.finfo(e4m3).max.astype(f32)\n",
    "\n",
    "assert jtu.is_cuda_compute_capability_at_least(\"9.0\")\n",
    "\n",
    "def check_fp8_call(lowered):\n",
    "  hlo = lowered.compile()\n",
    "  if re.search(r\"custom-call\\(f8e4m3fn.*, f8e4m3fn.*\", hlo.as_text()):\n",
    "    print(\"Fp8 call detected!\")\n",
    "  else:\n",
    "    print(\"No Fp8 call!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adc021f",
   "metadata": {},
   "source": [
    "## FLAX Low Level API\n",
    "\n",
    "The JAX dot operations (e.g. `jnp.dot`) support the FP8 dtype inputs. So it is\n",
    "legal to do the following call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54c374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.key(0)\n",
    "A = random.uniform(key, (16, 32))\n",
    "B = random.uniform(key, (32, 64))\n",
    "@jax.jit\n",
    "def dot_fp8(A, B):\n",
    "  return jnp.dot(A.astype(e4m3), B.astype(e4m3), preferred_element_type=f32)\n",
    "check_fp8_call(dot_fp8.lower(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb22878",
   "metadata": {},
   "source": [
    "However, there are two main issues with this approach. Firstly, `jnp.dot` does\n",
    "not accept scaling factors for the operands, defaulting to a scaling factor of\n",
    "1.0. Secondly, it does not support operands of mixed FP8 data types. For\n",
    "example, when the operands are E5M2 and E4M3, the dot product is performed using\n",
    "the promoted FP16 data type.\n",
    "\n",
    "In real-world scenarios, it is essential to specify scaling factors, either from\n",
    "calibration for inference or a user-defined algorithm during training.\n",
    "Additionally, it is common practice to use E5M2 for gradients and E4M3 for\n",
    "activations and kernels. These limitations make this method less practical for\n",
    "real-world applications.\n",
    "\n",
    "To address these limitations and create a more versatile FP8 dot product, we\n",
    "recommend leveraging XLA-FP8. Let's begin with a simple scaling strategy.\n",
    "\n",
    "\n",
    "### Current Scaling\n",
    "\n",
    "Scaling factors are usually defined as `scale = amax(x) / MAX`, where `amax` is\n",
    "an operation to find the absolute maximum value of the tensor, and `MAX` is the\n",
    "maximum value of the representable range of the target dtype. This scaling\n",
    "approach allows us to derive the scaling factors directly from the current\n",
    "operand tensors of the dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e746e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def dot_fp8(A, B):\n",
    "  A_scale = jnp.max(jnp.abs(A)) / E4M3_MAX\n",
    "  B_scale = jnp.max(jnp.abs(B)) / E4M3_MAX\n",
    "  A = fp8_ops.quantize_dequantize(A, e4m3, A_scale, f32)\n",
    "  B = fp8_ops.quantize_dequantize(B, e4m3, B_scale, f32)\n",
    "\n",
    "  C = jnp.dot(A, B)\n",
    "  return C\n",
    "\n",
    "C = dot_fp8(A, B)\n",
    "check_fp8_call(dot_fp8.lower(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aca6fe",
   "metadata": {},
   "source": [
    "As shown in the code, we perform fake quantization\n",
    "(`fp8_ops.quantize_dequantize`) on the operands of the dot product. Although the\n",
    "`jnp.dot` still processes higher-precision inputs, XLA detects this pattern and\n",
    "rewrites the dot operation as an FP8 dot call (e.g., cublasLt call for GPUs).\n",
    "This approach effectively mimics the first example but offers greater\n",
    "flexibility. We can control the input dtypes (both are set to E4M3 here, but we\n",
    "could use mixed E4M3 and E5M2) and define scaling factors, which XLA can detect\n",
    "and use in the dot backend.\n",
    "\n",
    "One major issue with the current scaling method is the overhead introduced by\n",
    "computing `A_scale` and `B_scale`, which requires additional loading of the\n",
    "operand tensors. To overcome this issue, we recommend the delayed scaling.\n",
    "\n",
    "### Delayed Scaling\n",
    "\n",
    "In delayed scaling, we use a scaling factor associated with an amax history. The\n",
    "scaling factor remains a scalar, but the amax history is a list that stores amax\n",
    "values from recent steps (e.g., 1024 steps). Both tensors are computed from\n",
    "previous steps and maintained in the model parameters.\n",
    "\n",
    "Fake quantization for delayed scaling is provided by `fp8_ops.in_qdq` for the\n",
    "activations and weights, and `fp8_ops.out_qdq` for the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf466308",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_scale = jnp.array(1.0)\n",
    "b_scale = jnp.array(1.0)\n",
    "g_scale = jnp.array(1.0)\n",
    "a_amax_hist = jnp.zeros((1024,))\n",
    "b_amax_hist = jnp.zeros((1024,))\n",
    "g_amax_hist = jnp.zeros((1024,))\n",
    "\n",
    "@jax.jit\n",
    "def dot_fp8(a, a_scale, a_amax_hist, b, b_scale, b_amax_hist,\n",
    "            g_scale, g_amax_hist):\n",
    "  a = fp8_ops.in_qdq(f32, e4m3, a, a_scale, a_amax_hist)\n",
    "  b = fp8_ops.in_qdq(f32, e4m3, b, b_scale, b_amax_hist)\n",
    "  \n",
    "  c = jnp.dot(a, b)\n",
    "  c = fp8_ops.out_qdq(f32, e5m2, c, g_scale, g_amax_hist)\n",
    "  return c\n",
    "\n",
    "C = dot_fp8(A, a_scale, a_amax_hist, B, b_scale, b_amax_hist,\n",
    "            g_scale, g_amax_hist)\n",
    "check_fp8_call(dot_fp8.lower(A, a_scale, a_amax_hist, B, b_scale, b_amax_hist,\n",
    "                             g_scale, g_amax_hist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bdc038",
   "metadata": {},
   "source": [
    "In this example, we first prepare three pairs of scaling factors and amax\n",
    "histories, treating them as results computed from previous steps. Then, we apply\n",
    "`fp8_ops.in_qdq` to the input operands of `jnp.dot`, followed by\n",
    "`fp8_ops.out_qdq` to the output of `jnp.dot`. Note the `fp8_ops.out_qdq` will\n",
    "apply fake quantization to the gradient of the output via custom_vjp functions.\n",
    "The new scaling factors and amax histories will be returned through their\n",
    "gradients, which will be covered in the next section.\n",
    "\n",
    "\n",
    "## FLAX High Level API\n",
    "With the FLAX library, incorporating FP8 operations into existing FLAX layers\n",
    "is a seamless process. Users don't need to manipulate the low-level APIs for\n",
    "quantization. Instead, they can integrate the provided custom FP8 dot\n",
    "(`fp8_ops.Fp8DotGeneralOp`) into FLAX layers using a straightforward\n",
    "\"code-injection\" approach. This custom operation encapsulates all FP8-related\n",
    "tasks, including the placement of quantization-dequantization ops, algorithms\n",
    "for updating scaling factors, and the selection of FP8 dtype combinations for\n",
    "forward and backward propagation.\n",
    "\n",
    "Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d9dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Dense(features=64, dot_general_cls=fp8_ops.Fp8DotGeneralOp)\n",
    "params = model.init(key, A)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(var, a): \n",
    "  c = model.apply(var, a)\n",
    "  return jnp.sum(c)\n",
    "\n",
    "check_fp8_call(train_step.lower(params, A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b0851",
   "metadata": {},
   "source": [
    "In this example, we simply set `dot_general_cls=fp8_ops.Fp8DotGeneralOp` to\n",
    "enable the Dense layer to utilize the FP8 dot operation. The usage of the model\n",
    "remains almost the same as before. The main difference is the addition of a new\n",
    "category of parameters: the sets of scaling factors and amax history. In the\n",
    "next section, we will explore how to update these parameters.\n",
    "\n",
    "## Manipulate FP8 params\n",
    "Let's first examine the data structure of `params`. In the code below, we redact\n",
    "the parameter values and then display the PyTree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873799fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_structure = flax.core.unfreeze(params).copy()\n",
    "params_structure = flax.traverse_util.flatten_dict(params_structure, sep='/')\n",
    "for key, value in params_structure.items():\n",
    "    params_structure[key] = '*'\n",
    "params_structure = flax.traverse_util.unflatten_dict(params_structure, sep='/')\n",
    "pprint.pprint(params_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031894dc",
   "metadata": {},
   "source": [
    "The output is as follows:\n",
    "\n",
    "```plaintext\n",
    "{'_overwrite_with_gradient': {'Fp8DotGeneralOp_0': {'input_amax_history': '*',\n",
    "                                                    'input_scale': '*',\n",
    "                                                    'kernel_amax_history': '*',\n",
    "                                                    'kernel_scale': '*',\n",
    "                                                    'output_grad_amax_history': '*',\n",
    "                                                    'output_grad_scale': '*'}},\n",
    " 'params': {'bias': '*', 'kernel': '*'}}\n",
    "```\n",
    "\n",
    "In addition to the expected `params`, there is an additional category called\n",
    "`_overwrite_with_gradient`. This category includes three pairs of `amax_history`\n",
    "and `scale` for the activation, kernel, and dot gradient, respectively.\n",
    "\n",
    "### Update gradient of FP8 params\n",
    "Now, we perform one training step to obtain the gradients and see how to use\n",
    "them to update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593fc35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_fn = jax.jit(jax.grad(train_step, (0, 1)))\n",
    "\n",
    "grads = step_fn(params, A)\n",
    "\n",
    "params = flax.core.unfreeze(params)\n",
    "params = flax.traverse_util.flatten_dict(params, sep='/')\n",
    "grads = flax.traverse_util.flatten_dict(grads[0], sep='/')\n",
    "\n",
    "for key, value in params.items():\n",
    "  if key.startswith('params'):\n",
    "    params[key] = value + 0.01 * grads[key]\n",
    "  if key.startswith('_overwrite_with_gradient'):\n",
    "    params[key] = grads[key]\n",
    "\n",
    "params = flax.traverse_util.unflatten_dict(params, sep='/')\n",
    "params = flax.core.freeze(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8e2153",
   "metadata": {},
   "source": [
    "The above code demonstrates how to update both `params` and\n",
    "`_overwrite_with_gradient`. For `params`, we use the formula `new_param =\n",
    "old_param + 0.01 * grads`, where `0.01` is the learning rate (or users can use\n",
    "whatever optimizers from `optax`). For `_overwrite_with_gradient`, we simply use\n",
    "the gradient to overwrite the old values.\n",
    "\n",
    "Note that `flax.training.train_state.TrainState` conveniently supports the\n",
    "category of `_overwrite_with_gradient`, so users do not need to modify their\n",
    "scripts if they don't use custom `TrainState`.\n",
    "\n",
    "## Accumulate gradient of FP8 params\n",
    "When the same parameter is used in a branched manner, the autograd mechanism\n",
    "will add their gradients from these branches. This is common in scenarios like\n",
    "pipeline parallelism, where each microbatch shares the same set of parameters\n",
    "for the minibatch. However, for the `_overwrite_with_gradient` parameters, this\n",
    "accumulation by addition is not meaningful. Instead, we prefer custom\n",
    "accumulation by taking the maximum value.\n",
    "\n",
    "To address this, we introduce a custom dtype `fp8_ops.fp32_max_grad`. The basic\n",
    "usage is demonstrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a86e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmax32 = fp8_ops.fp32_max_grad\n",
    "\n",
    "def reuse_fp8_param(x, y, scale, amax_history):\n",
    "  scale = scale.astype(fmax32)\n",
    "  amax_history = amax_history.astype(fmax32)\n",
    "\n",
    "  x = fp8_ops.in_qdq(f32, e4m3, x, scale, amax_history)\n",
    "  y = fp8_ops.in_qdq(f32, e4m3, y, scale, amax_history)\n",
    "  return x + y\n",
    "\n",
    "reuse_fp8_param_fn = jax.grad(reuse_fp8_param, (0, 1, 2, 3))\n",
    "reuse_fp8_param_fn = jax.jit(reuse_fp8_param_fn)\n",
    "\n",
    "_, _, new_ah, new_sf = reuse_fp8_param_fn(2.0, 3.0, a_scale, a_amax_hist)\n",
    "print(new_ah, new_sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2321a9bb",
   "metadata": {},
   "source": [
    "In this example, we first cast the `scale` and `amax_history` to\n",
    "`fp8_ops.fp32_max_grad` and then call `fp8_ops.in_qdq` twice using the same pair\n",
    "of `scale` and `amax_history`. During autograd, their gradients from each branch\n",
    "will be taken as the maximum, giving us the correct results of:\n",
    "\n",
    "```plaintext\n",
    "1.0 [3. 0. 0. ... 0. 0. 0.]\n",
    "```\n",
    "\n",
    "If we do not perform the type casting, we get the following result, meaning the\n",
    "gradients of the two branches are added:\n",
    "\n",
    "```plaintext\n",
    "2.0 [5. 0. 0. ... 0. 0. 0.]\n",
    "```\n",
    "\n",
    "This casting is already included if users choose to use the high-level APIs."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
