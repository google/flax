{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2a9f78765c0c"
   },
   "source": [
    "# Scale up Flax Modules on multiple devices with `pjit`\n",
    "\n",
    "This guide shows how to scale up [Flax Modules](https://flax.readthedocs.io/en/latest/developer_notes/module_lifecycle.html) on multiple devices and hosts using JAX's [`pjit`](https://jax.readthedocs.io/en/latest/jax.experimental.pjit.html#module-jax.experimental.pjit) and [`flax.linen.spmd`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#module-flax.linen.spmd).\n",
    "\n",
    "## Flax and `pjit`\n",
    "\n",
    "[`jax.experimental.pjit`](https://jax.readthedocs.io/en/latest/jax.experimental.pjit.html) provides a way to automatically compile and scale up JAX computations. `pjit` has the following benefits:\n",
    "\n",
    "* `pjit` has the similar interface of [`jax.jit`](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html) and works as a decorator on a function that needs to be compiled.\n",
    "* When using `pjit`, you can write code as if it runs on a single device, and `pjit` will automatically compile and run it on multiple devices using the [Single Program Multi Data (SPMD)](https://jax.readthedocs.io/en/latest/glossary.html#term-SPMD) paradigm. \n",
    "* With `pjit` you can state how the input and output of your code is partitioned across devices, and the compiler will figure out how to: 1) partition everything inside; and 2) compile inter-device communications.\n",
    "\n",
    "To learn more, refer to [JAX-101 pjit tutorial](https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html) and [JAX in multi-process environments](https://jax.readthedocs.io/en/latest/multi_process.html).\n",
    "\n",
    "Flax provides several functionalities that can help you use `pjit` on [Flax Modules](https://flax.readthedocs.io/en/latest/developer_notes/module_lifecycle.html), including:\n",
    "\n",
    "1. An interface to specify partitions of your data when defining [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#module).\n",
    "2. Utility functions to generate the partition information that `pjit` requires to run.\n",
    "3. An interface to customize your axis names called \"logical axis annotations\" to decouple both your Module code and partition plan to experiment with different partition layouts more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fa8ccbf573a"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Install Flax from HEAD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "867203db3bef",
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# Once Flax v0.6.4 is released, use `pip3 install flax`.\n",
    "! pip3 install -qq \"git+https://github.com/google/flax.git@main#egg=flax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9601432b448"
   },
   "source": [
    "## Imports\n",
    "\n",
    "Import some necessary dependencies.\n",
    "\n",
    "**Note:** This guide uses the `--xla_force_host_platform_device_count=8` flag to emulate multiple devices in a CPU environment in a Google Colab/Jupyter Notebook. Check out the [JAX-101 pjit tutorial](https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html#setup) to learn more about emulating a multi-device TPU environment (in which case you should ignore running `os.environ`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "f8f42d1174e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "b8da40732f0b"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "import jax\n",
    "\n",
    "from jax import lax, random, numpy as jnp\n",
    "\n",
    "import flax\n",
    "from flax import struct, traverse_util, linen as nn\n",
    "from flax.linen import spmd # Flax Linen SPMD.\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax.training import train_state, checkpoints\n",
    "\n",
    "import optax # Optax for common losses and optimizers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0d280def897"
   },
   "source": [
    "Next, import all the `pjit`-related libraries.\n",
    "\n",
    "> **Note:** [`jax.experimental.pjit`](https://jax.readthedocs.io/en/latest/jax.experimental.pjit.html) is still in the experimental package of JAX, so there may be changes in the API in future.\n",
    "\n",
    "1. Start a 2x4 device mesh (8 devices)—this is the same as the layout of [TPU v3-8](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#single_tpu_board).\n",
    "2. Annotate each axis with a name. A typical way to annotate axis names is `('data', 'model')`, where:\n",
    "  * `'data'`: the mesh dimension used for data-parallel sharding of the batch dimension of inputs and activations.\n",
    "  * `'model'`: the mesh dimension used for sharding parameters of the model across devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "684fe9fe13a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[CpuDevice(id=0) CpuDevice(id=1) CpuDevice(id=2) CpuDevice(id=3)]\n",
      " [CpuDevice(id=4) CpuDevice(id=5) CpuDevice(id=6) CpuDevice(id=7)]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Mesh(device_ids=array([[0, 1, 2, 3],\n",
       "       [4, 5, 6, 7]]), axis_names=('data', 'model'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax.experimental.pjit import pjit, with_sharding_constraint\n",
    "from jax.sharding import Mesh, PartitionSpec\n",
    "from jax.experimental import mesh_utils\n",
    "\n",
    "# Start a device mesh.\n",
    "device_mesh = mesh_utils.create_device_mesh((2, 4))\n",
    "print(device_mesh)\n",
    "# Annotate each axis with a name.\n",
    "mesh = Mesh(devices=device_mesh, axis_names=('data', 'model'))\n",
    "mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "307d39db6d94"
   },
   "source": [
    "## Define a layer\n",
    "\n",
    "Before defining a model, create an example layer called `DotReluDot` (by subclassing `flax.linen.Module`), which creates two parameters `W1` and `W2` for dot product multiplication, and uses the `jax.nn.relu` (ReLU) activation function in-between.\n",
    "\n",
    "To use this layer in `pjit` efficiently, apply the following APIs to annotate the parameters and intermediate variables correctly:\n",
    "\n",
    "1. Use [`flax.linen.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.with_partitioning.html#flax.linen.with_partitioning) to decorate the initializer function when creating parameters `W1` and `W2`.\n",
    "\n",
    "2. Apply [`pjit.with_sharding_constraint`](https://github.com/google/jax/blob/main/jax/_src/pjit.py#L1516) to annotate intermediate variables like `y` and `z` to force a particular sharding pattern under `pjit` when the ideal constraint is known.\n",
    "\n",
    "  * This step is optional, but can sometimes help auto-SPMD to partition efficiently. In the example below, the call is not required, because `pjit` will figure out the same sharding layout for `y` and `z` regardless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "b74c049968dc"
   },
   "outputs": [],
   "source": [
    "class DotReluDot(nn.Module):\n",
    "  depth: int\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    W1 = self.param(\n",
    "        'W1', \n",
    "        nn.with_partitioning(nn.initializers.xavier_normal(), (None, 'model')),\n",
    "        (x.shape[-1], self.depth))\n",
    "\n",
    "    y = jax.nn.relu(jnp.dot(x, W1))\n",
    "    # Force a local sharding annotation.\n",
    "    y = with_sharding_constraint(y, PartitionSpec('data', 'model'))\n",
    "\n",
    "    W2 = self.param(\n",
    "        'W2', \n",
    "        nn.with_partitioning(nn.initializers.xavier_normal(), ('model', None)),\n",
    "        (self.depth, x.shape[-1]))\n",
    "\n",
    "    z = jnp.dot(y, W2)\n",
    "    # Force a local sharding annotation.\n",
    "    z = with_sharding_constraint(z, PartitionSpec('data', None))\n",
    "\n",
    "    # Return a tuple to conform with the API `flax.linen.scan` as shown in the cell below.\n",
    "    return z, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbac5321c08e"
   },
   "source": [
    "Note that device axis names like `'data'`, `'model'` or `None` are passed into both `flax.linen.with_partitioning` and `pjit_with_sharding_constraint` API calls. This refers to how each dimension of this data should be sharded — either across one of the device mesh dimensions, or not sharded at all.\n",
    "\n",
    "For example:\n",
    "\n",
    "* When you define `W1` with shape `(x.shape[-1], self.depth)` and annotate as `(None, 'model')`:\n",
    "\n",
    "  * The first dimension (of length `x.shape[-1]`) will be replicated across all devices.\n",
    "  * The second dimension (of length `self.depth`) will be sharded over the `'model'` axis of the device mesh. This means `W1` will be sharded 4-way on devices `(0, 4)`, `(1, 5)`, `(2, 6)` and `(3, 7)`, on this dimension.\n",
    "\n",
    "* When you annotate the output `z` as `('data', None)`:\n",
    "\n",
    "  * The first dimension — the batch dimension — will be sharded over the `'data'` axis. This means half of the batch will be processed on devices `0-3` (first four devices), and another half on devices `4-7` (the remaining four devices).\n",
    "  * The second dimension — the data depth dimension — will be replicated across all devices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "b8389c11af79"
   },
   "source": [
    "## Define a model with `flax.linen.scan` lifted transformation\n",
    "\n",
    "This guide uses `flax.linen.scan` to demonstrate how [Flax lifted transforms](https://flax.readthedocs.io/en/latest/developer_notes/lift.html#supported-transformations), such as `scan`, can work together with [JAX `pjit`](https://jax.readthedocs.io/en/latest/jax.experimental.pjit.html).\n",
    "\n",
    "Having created `DotReluDot`, define the `MLP` model (by subclassing `flax.linen.Module`) as multiple layers of `DotReluDot`.\n",
    "\n",
    "To replicate identical layers, you can either use `flax.linen.scan`, or a for-loop:\n",
    "\n",
    "* `flax.linen.scan` can offer faster compilation times.\n",
    "* The for-loop can be faster on runtime.\n",
    "\n",
    "The code below shows how to apply both methods.\n",
    "\n",
    "**Note:** `flax.linen.scan` has another dimension for the parameters (the dimension over which `scan` is applied). You need to use the [`metadata_params`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.scan.html#flax.linen.scan) argument to annotate the partition of this dimension. Since the parameters inside your `DotReluDot` (a sub-`Module`) are already sharded along the `model` axis, you don't need to partition multiple layers across the `model` dimension here, and therefore you should denote it as `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "a0ea0dcccbc3"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  num_layers: int\n",
    "  depth: int\n",
    "  use_scan: bool\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    if self.use_scan:\n",
    "      x, _ = nn.scan(DotReluDot, length=self.num_layers, \n",
    "                     variable_axes={\"params\": 0},\n",
    "                     split_rngs={\"params\": True},\n",
    "                     metadata_params={nn.PARTITION_NAME: None}\n",
    "                     )(self.depth)(x)\n",
    "    else:\n",
    "      for i in range(self.num_layers):\n",
    "        x, _ = DotReluDot(self.depth)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b3abfef359d"
   },
   "source": [
    "## Specify sharding (includes initialization and `TrainState` creation)\n",
    "\n",
    "Next, generate the [`jax.sharding.PartitionSpec`](https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html?#more-information-on-partitionspec) that `pjit` should receive as annotations of _input_ and _output_ data. `PartitionSpec` is a tuple of 2 axes (in a 2x4 mesh). To learn more, refer to [JAX-101: Introduction to `pjit`](https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html).\n",
    "\n",
    "### Specify the input\n",
    "\n",
    "For data parallelism, you can shard the batched _input_ `x` across the `data` axis by denoting the batch axis as `data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "4b8472d462f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PartitionSpec('data', None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_spec = PartitionSpec('data', None)  # dimensions: (batch, length)\n",
    "x_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06d134795ae1"
   },
   "source": [
    "### Generate a `PartitionSpec` for the output\n",
    "\n",
    "Next, generate a [`PartitionSpec`](https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html?#more-information-on-partitionspec) for the _output_, you need to use some actual output as a reference.\n",
    "\n",
    "1. Instantiate a model.\n",
    "2. Evaluate `model.init` abstractly using [`jax.eval_shape`](https://jax.readthedocs.io/en/latest/_autosummary/jax.eval_shape.html).\n",
    "3. Use [`flax.linen.get_partition_spec`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.get_partition_spec.html) to automatically generate the `PartitionSpec`.\n",
    "\n",
    "The code below shows how to get the output spec if you use `flax.training.train_state` to carry out your initialization and training steps, in which case your `pjit`ted function will output a `TrainState`. \n",
    "\n",
    "(In a simpler case, people might choose the variable dict as in `variables = model.init(k, x)` as their `pjit`ted function's output. That works too.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "8b913a2e57d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainState(step=None, apply_fn=<bound method Module.apply of MLP(\n",
       "    # attributes\n",
       "    num_layers = 4\n",
       "    depth = 1024\n",
       "    use_scan = True\n",
       ")>, params=FrozenDict({\n",
       "    ScanDotReluDot_0: {\n",
       "        W1: PartitionSpec(None, None, 'model'),\n",
       "        W2: PartitionSpec(None, 'model', None),\n",
       "    },\n",
       "}), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x14f96c1f0>, update=<function chain.<locals>.update_fn at 0x14f96c160>), opt_state=(ScaleByAdamState(count=None, mu=FrozenDict({\n",
       "    ScanDotReluDot_0: {\n",
       "        W1: PartitionSpec(None, None, 'model'),\n",
       "        W2: PartitionSpec(None, 'model', None),\n",
       "    },\n",
       "}), nu=FrozenDict({\n",
       "    ScanDotReluDot_0: {\n",
       "        W1: PartitionSpec(None, None, 'model'),\n",
       "        W2: PartitionSpec(None, 'model', None),\n",
       "    },\n",
       "})), EmptyState()))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLP hyperparameters.\n",
    "BATCH, LAYERS, DEPTH, USE_SCAN = 8, 4, 1024, True\n",
    "# Create fake inputs.\n",
    "x = jnp.ones((BATCH, DEPTH))\n",
    "# Initialize a PRNG key.\n",
    "k = random.PRNGKey(0)\n",
    "\n",
    "# Create an Optax optimizer.\n",
    "optimizer = optax.adam(learning_rate=0.001)\n",
    "# Instantiate the model.\n",
    "model = MLP(LAYERS, DEPTH, USE_SCAN)\n",
    "\n",
    "# A functional way of model initialization.\n",
    "def init_fn(k, x, model, optimizer):\n",
    "  variables = model.init(k, x) # Initialize the model.\n",
    "  state = train_state.TrainState.create( # Create a `TrainState`.\n",
    "    apply_fn=model.apply,\n",
    "    params=variables['params'],\n",
    "    tx=optimizer)\n",
    "  return state\n",
    "\n",
    "with mesh:\n",
    "  # Create an abstract closure to wrap the function before feeding it in\n",
    "  # because `jax.eval_shape` only takes pytrees as arguments`.\n",
    "  abstract_variables = jax.eval_shape(\n",
    "      functools.partial(init_fn, model=model, optimizer=optimizer), k, x)\n",
    "# This `state_spec` has the same pytree structure as the output\n",
    "# of the `init_fn`.\n",
    "state_spec = nn.get_partition_spec(abstract_variables)\n",
    "state_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ec24614050b"
   },
   "source": [
    "## Apply `pjit` to compile the code\n",
    "\n",
    "Now you can apply JAX [`pjit`](https://jax.readthedocs.io/en/latest/jax.experimental.pjit.html#module-jax.experimental.pjit) to your `init_fn` in a similar fashion as [`jax.jit`](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html) but with two extra arguments: `in_axis_resources` and `out_axis_resources`.\n",
    "\n",
    "You need to add a `with mesh:` context when running a `pjit`ted function, so that it can refer to `mesh` (an instance of `jax.sharding.Mesh`) to allocate data on devices correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "a298c5d03c0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainState(step=(), apply_fn=<bound method Module.apply of MLP(\n",
       "    # attributes\n",
       "    num_layers = 4\n",
       "    depth = 1024\n",
       "    use_scan = True\n",
       ")>, params=FrozenDict({\n",
       "    ScanDotReluDot_0: {\n",
       "        W1: Partitioned(value=(4, 1024, 1024), names=(None, None, 'model')),\n",
       "        W2: Partitioned(value=(4, 1024, 1024), names=(None, 'model', None)),\n",
       "    },\n",
       "}), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x14f96c1f0>, update=<function chain.<locals>.update_fn at 0x14f96c160>), opt_state=(ScaleByAdamState(count=(), mu=FrozenDict({\n",
       "    ScanDotReluDot_0: {\n",
       "        W1: Partitioned(value=(4, 1024, 1024), names=(None, None, 'model')),\n",
       "        W2: Partitioned(value=(4, 1024, 1024), names=(None, 'model', None)),\n",
       "    },\n",
       "}), nu=FrozenDict({\n",
       "    ScanDotReluDot_0: {\n",
       "        W1: Partitioned(value=(4, 1024, 1024), names=(None, None, 'model')),\n",
       "        W2: Partitioned(value=(4, 1024, 1024), names=(None, 'model', None)),\n",
       "    },\n",
       "})), EmptyState()))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pjit_init_fn = pjit(init_fn,\n",
    "                    static_argnums=(2, 3),\n",
    "                    in_axis_resources=(PartitionSpec(None), x_spec),  # PRNG key and x\n",
    "                    out_axis_resources=state_spec\n",
    "                    )\n",
    "with mesh:\n",
    "  initialized_state = pjit_init_fn(k, x, model, optimizer)\n",
    "jax.tree_map(jnp.shape, initialized_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f74b009f11f"
   },
   "source": [
    "## Inspect the Module output\n",
    "\n",
    "Note that in the output of `initialized_state`, the `params` `W1` and `W2` are of type [`flax.linen.Partitioned`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.Partitioned.html). This is a wrapper around the actual `jax.Array` that allows Flax to record metadata associated with it. You can access the raw `jax.Array` by adding `.value` or running `.unbox()`.\n",
    "\n",
    "You can also check the underlying [`jax.sharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html) of the JAX array, which gives a hint on the way it is partitioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "19243982c892"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'flax.core.meta.Partitioned'>\n",
      "<class 'jaxlib.xla_extension.Array'>\n",
      "(4, 1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(type(initialized_state.params['ScanDotReluDot_0']['W1']))\n",
    "print(type(initialized_state.params['ScanDotReluDot_0']['W1'].value))\n",
    "print(initialized_state.params['ScanDotReluDot_0']['W1'].value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "2067c419a826"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpShardingSharding({devices=[1,1,4,2]0,4,1,5,2,6,3,7 last_tile_dim_replicate})\n"
     ]
    }
   ],
   "source": [
    "print(initialized_state.params['ScanDotReluDot_0']['W1'].value.sharding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "273547d3ab89"
   },
   "source": [
    "You can use [`jax.tree_map`](https://jax.readthedocs.io/en/latest/_autosummary/jax.tree_util.tree_map.html) to perform mass computation on a dict of boxed params, in the same way as on a dict of JAX arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "29b3dae156a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict({\n",
      "    W1: Partitioned(value=(4, 1024, 1024), names=(None, None, 'model')),\n",
      "    W2: Partitioned(value=(4, 1024, 1024), names=(None, 'model', None)),\n",
      "})\n",
      "<class 'jaxlib.xla_extension.Array'>\n",
      "(4, 1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "diff = jax.tree_map(\n",
    "    lambda a, b: a - b, \n",
    "    initialized_state.params['ScanDotReluDot_0'], initialized_state.params['ScanDotReluDot_0'])\n",
    "print(jax.tree_map(jnp.shape, diff))\n",
    "diff_array = diff['W1'].unbox()\n",
    "print(type(diff_array))\n",
    "print(diff_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7e1ccb14c6b"
   },
   "source": [
    "## Apply `pjit` to the train step and inference \n",
    "\n",
    "Now, you create a `pjit`ted training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "4e3cc300cfee"
   },
   "outputs": [],
   "source": [
    "def train_step(state, x):\n",
    "  # A fake loss function.\n",
    "  def loss_unrolled(params):\n",
    "    y = model.apply({'params': params}, x)\n",
    "    return y.sum()\n",
    "  grad_fn = jax.grad(loss_unrolled)\n",
    "  grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  return state\n",
    "\n",
    "pjit_step_fn = pjit(train_step,\n",
    "                    in_axis_resources=(state_spec, x_spec),  # input annotations\n",
    "                    out_axis_resources=state_spec,           # output annotations\n",
    "                    )\n",
    "with mesh:\n",
    "  new_state = pjit_step_fn(initialized_state, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bae79e2e71b"
   },
   "source": [
    "Apply `pjit` to inference. Note that, similar to `jax.jit`, you can use a decorator like `@functools.partial(pjit, ...)` to directly compile your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "c9264a48b9ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jaxlib.xla_extension.Array'>\n",
      "float32\n",
      "(8, 1024)\n"
     ]
    }
   ],
   "source": [
    "@functools.partial(pjit, in_axis_resources=(state_spec, x_spec), out_axis_resources=x_spec)\n",
    "def pjit_apply_fn(state, x):\n",
    "  return state.apply_fn({'params': state.params}, x)\n",
    "\n",
    "with mesh:\n",
    "  y = pjit_apply_fn(new_state, x)\n",
    "print(type(y))\n",
    "print(y.dtype)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7daa9e6e6eb4"
   },
   "source": [
    "## Profiling\n",
    "\n",
    "If you are running on a TPU pod or a pod slice, you can use a custom `block_all` utility function, as defined below, to measure the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "a68d7cb2eb89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166 ms ± 5.72 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "def block_all(xs):\n",
    "  jax.tree_map(lambda x: x.block_until_ready(), xs)\n",
    "  return xs\n",
    "\n",
    "with mesh:\n",
    "  new_state = block_all(pjit_step_fn(initialized_state, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51420b514d53"
   },
   "source": [
    "## Logical axis annotation\n",
    "\n",
    "JAX auto SPMD encourages users to explore different sharding layouts to find the optimal one. To this end, in Flax you actually can annotate the dimensions of any data with more descriptive axis names (not just device mesh axis names like `'data'` and `'model'`). \n",
    "\n",
    "The `LogicalDotReluDot` and `LogicalMLP` Module definition below are similar to the Modules you created earlier, except for the following:\n",
    "\n",
    "1. All axes are annotated with more concrete, meaningful names, such as `'embed'`, `'hidden'`, `'batch'` and `'layer'`. These names are referred to as _logical axis names_ in Flax. They make the dimensional changes inside model definitions more readable.\n",
    "\n",
    "2. `flax.linen.spmd.with_logical_partitioning` replaces `flax.linen.with_partitioning`; and `flax.linen.spmd.with_logical_constraint` replaces `pjit.with_sharding_constraint`, to recognize the logical axis names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "a26f85a9e772"
   },
   "outputs": [],
   "source": [
    "class LogicalDotReluDot(nn.Module):\n",
    "  depth: int\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    W1 = self.param(\n",
    "        'W1', \n",
    "        spmd.with_logical_partitioning(nn.initializers.xavier_normal(), ('embed', 'hidden')),\n",
    "        (x.shape[-1], self.depth)) \n",
    "\n",
    "    y = jax.nn.relu(jnp.dot(x, W1))\n",
    "    # Force a local sharding annotation.\n",
    "    y = spmd.with_logical_constraint(y, ('batch', 'hidden'))\n",
    "\n",
    "    W2 = self.param(\n",
    "        'W2', \n",
    "        spmd.with_logical_partitioning(nn.initializers.xavier_normal(), ('hidden', 'embed')),\n",
    "        (self.depth, x.shape[-1]))\n",
    "\n",
    "    z = jnp.dot(y, W2)\n",
    "    # Force a local sharding annotation.\n",
    "    z = spmd.with_logical_constraint(z, ('batch', 'embed'))\n",
    "    return z, None\n",
    "\n",
    "class LogicalMLP(nn.Module):\n",
    "  num_layers: int\n",
    "  depth: int\n",
    "  use_scan: bool\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    if self.use_scan:\n",
    "      x, _ = nn.scan(LogicalDotReluDot, length=self.num_layers, \n",
    "                    variable_axes={\"params\": 0},\n",
    "                    split_rngs={\"params\": True},\n",
    "                    metadata_params={nn.PARTITION_NAME: 'layer'}\n",
    "                    )(self.depth)(x)\n",
    "    else:\n",
    "      for i in range(self.num_layers):\n",
    "        x, _ = DotReluDot(self.depth)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0de93ec6cbd6"
   },
   "source": [
    "The `LogicalMLP` model definition generates a set of `PartitionSpec` with logical axis names.\n",
    "\n",
    "Repeat the steps from earlier: instantiate a model, evaluate the `init_fn` abstractly, and use `flax.linen.get_partition_spec` to automatically generate the `PartitionSpec`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "14db7a1e30fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainState(step=None, apply_fn=<bound method Module.apply of LogicalMLP(\n",
       "    # attributes\n",
       "    num_layers = 4\n",
       "    depth = 1024\n",
       "    use_scan = True\n",
       ")>, params=FrozenDict({\n",
       "    ScanLogicalDotReluDot_0: {\n",
       "        W1: PartitionSpec('layer', 'embed', 'hidden'),\n",
       "        W2: PartitionSpec('layer', 'hidden', 'embed'),\n",
       "    },\n",
       "}), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x14f96c1f0>, update=<function chain.<locals>.update_fn at 0x14f96c160>), opt_state=(ScaleByAdamState(count=None, mu=FrozenDict({\n",
       "    ScanLogicalDotReluDot_0: {\n",
       "        W1: PartitionSpec('layer', 'embed', 'hidden'),\n",
       "        W2: PartitionSpec('layer', 'hidden', 'embed'),\n",
       "    },\n",
       "}), nu=FrozenDict({\n",
       "    ScanLogicalDotReluDot_0: {\n",
       "        W1: PartitionSpec('layer', 'embed', 'hidden'),\n",
       "        W2: PartitionSpec('layer', 'hidden', 'embed'),\n",
       "    },\n",
       "})), EmptyState()))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logical_model = LogicalMLP(LAYERS, DEPTH, USE_SCAN)\n",
    "logical_abstract_variables = jax.eval_shape(\n",
    "    functools.partial(init_fn, model=logical_model, optimizer=optimizer), k, x)\n",
    "logical_output_spec = nn.get_partition_spec(logical_abstract_variables)\n",
    "logical_output_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1c9b74e50b9"
   },
   "source": [
    "To allow the device mesh to take your model correctly, you need to decide which of these logical axis names are mapped to the device axis `'data'` or `'model'`. This rule is a list of (`logical_axis_name`, `device_axis_name`) tuples, and `jax.linen.spmd.logical_to_mesh` will convert them to the spec that `pjit` accepts.\n",
    "\n",
    "This allows you to change the rules and try out new partition layouts without modifying the model definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "711cb4bde093"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainState(step=None, apply_fn=<bound method Module.apply of LogicalMLP(\n",
       "    # attributes\n",
       "    num_layers = 4\n",
       "    depth = 1024\n",
       "    use_scan = True\n",
       ")>, params=FrozenDict({\n",
       "    ScanLogicalDotReluDot_0: {\n",
       "        W1: PartitionSpec(None, None, 'model'),\n",
       "        W2: PartitionSpec(None, 'model', None),\n",
       "    },\n",
       "}), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x14f96c1f0>, update=<function chain.<locals>.update_fn at 0x14f96c160>), opt_state=(ScaleByAdamState(count=None, mu=FrozenDict({\n",
       "    ScanLogicalDotReluDot_0: {\n",
       "        W1: PartitionSpec(None, None, 'model'),\n",
       "        W2: PartitionSpec(None, 'model', None),\n",
       "    },\n",
       "}), nu=FrozenDict({\n",
       "    ScanLogicalDotReluDot_0: {\n",
       "        W1: PartitionSpec(None, None, 'model'),\n",
       "        W2: PartitionSpec(None, 'model', None),\n",
       "    },\n",
       "})), EmptyState()))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unspecified rule means unsharded by default, so no need to specify `('embed', None)` and `('layer', None)`.\n",
    "rules = (('batch', 'data'),\n",
    "         ('hidden', 'model'))\n",
    "\n",
    "logical_state_spec = spmd.logical_to_mesh(logical_output_spec, rules)\n",
    "logical_state_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58475fffb2de"
   },
   "source": [
    "You can verify that the `logical_state_spec` here has the same content as `state_spec` in the previous (\"non-logical\") example. This will be the `out_axis_resources` you specify when creating `pjit`ted functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "589ff774bb4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_spec.params['ScanDotReluDot_0'] == logical_state_spec.params['ScanLogicalDotReluDot_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "77e07a0ab309"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainState(step=(), apply_fn=<bound method Module.apply of LogicalMLP(\n",
       "    # attributes\n",
       "    num_layers = 4\n",
       "    depth = 1024\n",
       "    use_scan = True\n",
       ")>, params=FrozenDict({\n",
       "    ScanLogicalDotReluDot_0: {\n",
       "        W1: LogicallyPartitioned(value=(4, 1024, 1024), names=('layer', 'embed', 'hidden')),\n",
       "        W2: LogicallyPartitioned(value=(4, 1024, 1024), names=('layer', 'hidden', 'embed')),\n",
       "    },\n",
       "}), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x14f96c1f0>, update=<function chain.<locals>.update_fn at 0x14f96c160>), opt_state=(ScaleByAdamState(count=(), mu=FrozenDict({\n",
       "    ScanLogicalDotReluDot_0: {\n",
       "        W1: LogicallyPartitioned(value=(4, 1024, 1024), names=('layer', 'embed', 'hidden')),\n",
       "        W2: LogicallyPartitioned(value=(4, 1024, 1024), names=('layer', 'hidden', 'embed')),\n",
       "    },\n",
       "}), nu=FrozenDict({\n",
       "    ScanLogicalDotReluDot_0: {\n",
       "        W1: LogicallyPartitioned(value=(4, 1024, 1024), names=('layer', 'embed', 'hidden')),\n",
       "        W2: LogicallyPartitioned(value=(4, 1024, 1024), names=('layer', 'hidden', 'embed')),\n",
       "    },\n",
       "})), EmptyState()))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logical_pjit_init_fn = pjit(init_fn,\n",
    "                            static_argnums=(2, 3),\n",
    "                            in_axis_resources=(PartitionSpec(None), x_spec),  # RNG key and x\n",
    "                            out_axis_resources=logical_state_spec\n",
    "                            )\n",
    "with mesh:\n",
    "  logical_initialized_state = logical_pjit_init_fn(k, x, logical_model, optimizer)\n",
    "jax.tree_map(jnp.shape, logical_initialized_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ae1754a3031d"
   },
   "source": [
    "## When to use device axis / logical axis\n",
    "\n",
    "Choosing when to use a device or logical axis depends on how much you want to control the partitioning of your model.\n",
    "\n",
    "If you want a very simple model, or you are very confident of your way of partitioning, defining it with __device mesh axis__ can potentially save you a few extra lines of code of converting the logical naming back to the device naming.\n",
    "\n",
    "On the other hand, the __logical naming__ helpers are useful for exploring different sharding layouts. Use this if you want to experiment around and find the most optimal partition layout for your model.\n",
    "\n",
    "In really advanced use cases, you may have more complicated sharding patterns that require annotating *activation* dimension names differently from *parameter* dimension names. When people wish to have more fine-grained control on manual mesh assignments, directly using __device axis names__ could be more helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "576bdd5cd782"
   },
   "source": [
    "## Save the data\n",
    "\n",
    "You can use [`flax.training.checkpoints`](https://flax.readthedocs.io/en/latest/_modules/flax/training/checkpoints.html) to save the cross-device array, as shown in the [Save and load checkpoints guide - Multi-host/multi-process checkpointing](https://flax.readthedocs.io/en/latest/guides/use_checkpointing.html#multi-host-multi-process-checkpointing). This is especially required if you are running on a multi-host environment (for example, a TPU pod).\n",
    "\n",
    "Keep in mind that to restore the arrays to the desired partition, you need to provide a sample `target` pytree that has the same structure and has the desired `PartitionSpec` in place for each JAX array. The `PartitionSpec` you use to restore the array doesn't necessarily need to be the same as the ones you used to store the array."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
