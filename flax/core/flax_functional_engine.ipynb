{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0SPwYS9dtYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import functools\n",
        "import jax\n",
        "from jax import numpy as jnp, random, lax\n",
        "import numpy as onp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n9cxyCzluvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flax import nn, struct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L7YCrobkfzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flax.core.scope import Scope, init, apply, Array, group_kinds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDLGb3iGkjoL",
        "colab_type": "code",
        "outputId": "2558605e-e485-407e-b062-74d31cc49f1e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1590673431275,
          "user_tz": -120,
          "elapsed": 1116,
          "user": {
            "displayName": "Jonathan Heek",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhqRcoo1w0woYaM99jSyWQaD-qfmHmeDpXHzHZd=s64",
            "userId": "00491914421152177709"
          }
        },
        "colab": {
          "height": 136
        }
      },
      "source": [
        "def dense(scope: Scope, inputs: Array, features: int, bias: bool = True,\n",
        "          kernel_init=nn.linear.default_kernel_init,\n",
        "          bias_init=nn.initializers.zeros):\n",
        "  kernel = scope.param('kernel', kernel_init, (inputs.shape[-1], features))\n",
        "  y = jnp.dot(inputs, kernel)\n",
        "  if bias:\n",
        "    y += scope.param('bias', bias_init, (features,))\n",
        "  return y\n",
        "\n",
        "model_fn = functools.partial(dense, features=3)\n",
        "\n",
        "x = jnp.ones((1, 2))\n",
        "y, params = init(model_fn)(random.PRNGKey(0), x)\n",
        "print(params)\n",
        "\n",
        "def mlp(scope: Scope, inputs: Array, features: int):\n",
        "  hidden = scope.child(dense, 'hidden')(inputs, features)\n",
        "  hidden = nn.relu(hidden)\n",
        "  return dense(scope.push('out'), hidden, 1)\n",
        "\n",
        "init(mlp)(random.PRNGKey(0), x, features=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params = frozendict({'hidden': {'kernel', 'bias'}})\n",
        "mlp_scope = Scope(params)\n",
        "# hidden_scope = mlp_scope.push('hidden')\n",
        "hidden_scope = Scope({})\n",
        "hidden_scope.get_variable('kernel') # no kernel\n",
        "mlp_scope.add_child(hidden_scope, 'hidden')\n",
        "hidden_scope.get_variable('kernel') # kernel\n",
        "\n",
        "def mlp(scope, x):\n",
        "    hidden_scope = Scope(None)\n",
        "    scope.add_child(hidden_scope, 'hidden')\n",
        "    \n",
        "apply(mlp, params)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHoB5lomZp9s",
        "colab_type": "code",
        "outputId": "d2898913-ece5-434e-eedf-249324df1775",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1590672862325,
          "user_tz": -120,
          "elapsed": 1003,
          "user": {
            "displayName": "Jonathan Heek",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhqRcoo1w0woYaM99jSyWQaD-qfmHmeDpXHzHZd=s64",
            "userId": "00491914421152177709"
          }
        },
        "colab": {
          "height": 187
        }
      },
      "source": [
        "from typing import Iterable, Any\n",
        "\n",
        "def dense_general(\n",
        "    scope: Scope, inputs: Array, features: int, axis = -1,\n",
        "    batch_dims=(), bias=True, dtype=jnp.float32,\n",
        "    kernel_init=nn.linear.default_kernel_init, bias_init=nn.initializers.zeros,\n",
        "    precision=None):\n",
        "  \"\"\"Applies a linear transformation to the inputs along multiple dimensions.\n",
        "\n",
        "  Args:\n",
        "    scope: module scope\n",
        "    inputs: The nd-array to be transformed.\n",
        "    features: tuple with numbers of output features.\n",
        "    axis: tuple with axes to apply the transformation on.\n",
        "    batch_dims: tuple with batch axes.\n",
        "    bias: whether to add a bias to the output (default: True).\n",
        "    dtype: the dtype of the computation (default: float32).\n",
        "    kernel_init: initializer function for the weight matrix.\n",
        "    bias_init: initializer function for the bias.\n",
        "    precision: numerical precision of the computation see `jax.lax.Precision`\n",
        "      for details.\n",
        "  Returns:\n",
        "    The transformed input.\n",
        "  \"\"\"\n",
        "  inputs = jnp.asarray(inputs, dtype)\n",
        "  features = _as_tuple(features)\n",
        "  axis = _as_tuple(axis)\n",
        "  batch_dims = _as_tuple(batch_dims)\n",
        "  \n",
        "  if batch_dims:\n",
        "    max_dim = onp.max(batch_dims)\n",
        "    if set(batch_dims) != set(range(max_dim + 1)):\n",
        "      raise ValueError(f'batch_dims {batch_dims} must be consecutive leading '\n",
        "                       'dimensions starting from 0.')\n",
        "      \n",
        "  ndim = inputs.ndim\n",
        "  n_batch_dims = len(batch_dims)\n",
        "  axis = _normalize_axes(axis, ndim)\n",
        "  batch_dims = _normalize_axes(batch_dims, ndim)\n",
        "  n_axis, n_features = len(axis), len(features)\n",
        "\n",
        "  batch_shape = tuple([inputs.shape[ax] for ax in batch_dims])\n",
        "  kernel_shape = tuple([inputs.shape[ax] for ax in axis]) + features\n",
        "  kernel = scope.param('kernel', kernel_init, batch_shape + kernel_shape)\n",
        "  kernel = jnp.asarray(kernel, dtype)\n",
        "\n",
        "  batch_ind = tuple(range(n_batch_dims))\n",
        "  contract_ind = tuple(range(n_batch_dims, n_axis + n_batch_dims))\n",
        "  out = lax.dot_general(inputs,\n",
        "                        kernel,\n",
        "                        ((axis, contract_ind), (batch_dims, batch_ind)),\n",
        "                        precision=precision)\n",
        "  if bias:\n",
        "    bias = scope.param('bias', bias_init, batch_shape + features)\n",
        "\n",
        "    # Reshape bias for broadcast.\n",
        "    expand_dims = sorted(\n",
        "        set(range(inputs.ndim)) - set(axis) - set(batch_dims))\n",
        "    for ax in expand_dims:\n",
        "      bias = jnp.expand_dims(bias, ax)\n",
        "    bias = jnp.asarray(bias, dtype)\n",
        "    out = out + bias\n",
        "  return out\n",
        "\n",
        "def _as_tuple(x):\n",
        "  if isinstance(x, Iterable):\n",
        "    return tuple(x)\n",
        "  return (x,)\n",
        "\n",
        "def _normalize_axes(axes, ndim):\n",
        "  # A tuple by convention. len(axes_tuple) then also gives the rank efficiently.\n",
        "  return tuple([ax if ax >= 0 else ndim + ax for ax in axes])\n",
        "\n",
        "init(dense_general)(random.PRNGKey(0), jnp.ones((2, 3,)), features=1, batch_dims=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHKL_QOBa-IK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@struct.dataclass\n",
        "class CacheEntry:\n",
        "  key: onp.ndarray\n",
        "  value: onp.ndarray\n",
        "  i: onp.ndarray\n",
        "\n",
        "def multi_head_dot_product_attention(\n",
        "    scope: Scope,\n",
        "          inputs_q,\n",
        "          inputs_kv,\n",
        "          num_heads,\n",
        "          dtype=jnp.float32,\n",
        "          qkv_features=None,\n",
        "          out_features=None,\n",
        "          attention_axis=None,\n",
        "          causal_mask=False,\n",
        "          padding_mask=None,\n",
        "          key_padding_mask=None,\n",
        "          segmentation=None,\n",
        "          key_segmentation=None,\n",
        "          cache=False,\n",
        "          broadcast_dropout=True,\n",
        "          dropout_rng=None,\n",
        "          dropout_rate=0.,\n",
        "          deterministic=False,\n",
        "          precision=None,\n",
        "          kernel_init=nn.attention.default_kernel_init,\n",
        "          bias_init=nn.initializers.zeros,\n",
        "          bias=True,\n",
        "          attention_fn=nn.attention.dot_product_attention):\n",
        "  \"\"\"Applies multi-head dot product attention on the input data.\n",
        "\n",
        "  Projects the inputs into multi-headed query, key, and value vectors,\n",
        "  applies dot-product attention and project the results to an output vector.\n",
        "\n",
        "  This can be used for encoder-decoder attention by specifying both `inputs_q`\n",
        "  and `inputs_kv` orfor self-attention by only specifying `inputs_q` and\n",
        "  setting `inputs_kv` to None.\n",
        "\n",
        "  Args:\n",
        "    inputs_q: input queries of shape `[bs, dim1, dim2, ..., dimN, features]`.\n",
        "    inputs_kv: key/values of shape `[bs, dim1, dim2, ..., dimN, features]`\n",
        "      or None for self-attention, inn which case key/values will be derived\n",
        "      from inputs_q.\n",
        "    num_heads: number of attention heads. Features (i.e. inputs_q.shape[-1])\n",
        "      should be divisible by the number of heads.\n",
        "    dtype: the dtype of the computation (default: float32)\n",
        "    qkv_features: dimension of the key, query, and value.\n",
        "    out_features: dimension of the last projection\n",
        "    attention_axis: axes over which the attention is applied ( 'None' means\n",
        "      attention over all axes, but batch, heads, and features).\n",
        "    causal_mask: boolean specifying whether to apply a causal mask on the\n",
        "      attention weights. If True, the output at timestep `t` will not depend\n",
        "      on inputs at timesteps strictly greater than `t`.\n",
        "    padding_mask: boolean specifying query tokens that are pad token.\n",
        "    key_padding_mask: boolean specifying key-value tokens that are pad token.\n",
        "    segmentation: segment indices for packed inputs_q data.\n",
        "    key_segmentation: segment indices for packed inputs_kv data.\n",
        "    cache: an instance of `flax.nn.attention.Cache` used for efficient\n",
        "      autoregressive decoding.\n",
        "    broadcast_dropout: bool: use a broadcasted dropout along batch dims.\n",
        "    dropout_rng: JAX PRNGKey: to be used for dropout\n",
        "    dropout_rate: dropout rate\n",
        "    deterministic: bool, deterministic or not (to apply dropout)\n",
        "    precision: numerical precision of the computation see `jax.lax.Precision`\n",
        "      for details.\n",
        "    kernel_init: initializer for the kernel of the Dense layers.\n",
        "    bias_init: initializer for the bias of the Dense layers.\n",
        "    bias: bool: whether pointwise QKVO dense transforms use bias.\n",
        "    attention_fn: dot_product_attention or compatible function. Accepts\n",
        "    query, key, value, and returns output of shape\n",
        "    `[bs, dim1, dim2, ..., dimN,, num_heads, value_channels]``\n",
        "\n",
        "  Returns:\n",
        "    output of shape `[bs, dim1, dim2, ..., dimN, features]`.\n",
        "  \"\"\"\n",
        "\n",
        "  assert causal_mask or not cache, (\n",
        "      'Caching is only support for causal attention.')\n",
        "\n",
        "  if inputs_kv is None:\n",
        "    inputs_kv = inputs_q\n",
        "\n",
        "  if attention_axis is None:\n",
        "    attention_axis = tuple(range(1, inputs_q.ndim - 1))\n",
        "\n",
        "  features = out_features or inputs_q.shape[-1]\n",
        "  qkv_features = qkv_features or inputs_q.shape[-1]\n",
        "\n",
        "  assert qkv_features % num_heads == 0, (\n",
        "      'Memory dimension must be divisible by number of heads.')\n",
        "  head_dim = qkv_features // num_heads\n",
        "\n",
        "  dense = functools.partial(dense_general,\n",
        "      axis=-1,\n",
        "      dtype=dtype,\n",
        "      features=(num_heads, head_dim),\n",
        "      kernel_init=kernel_init,\n",
        "      bias_init=bias_init,\n",
        "      bias=bias,\n",
        "      precision=precision)\n",
        "  # project inputs_q to multi-headed q/k/v\n",
        "  # dimensions are then [bs, dims..., n_heads, n_features_per_head]\n",
        "  query = scope.child(dense, 'query')(inputs_q)\n",
        "  key = scope.child(dense, 'key')(inputs_kv)\n",
        "  value = scope.child(dense, 'value')(inputs_kv)\n",
        "\n",
        "  if cache:\n",
        "    if not scope.has_variable('cache', 'entry'):\n",
        "      ndim, tail_shape = (key.ndim, key.shape[-2:])\n",
        "      def init_fn(shape):\n",
        "        full_shape = shape + tail_shape\n",
        "        if len(full_shape) != ndim:\n",
        "          raise ValueError('Shape should be a tuple with the shape of the batch'\n",
        "                          'and attention dims.')\n",
        "        return CacheEntry(\n",
        "            key=jnp.zeros(full_shape),\n",
        "            value=jnp.zeros(full_shape),\n",
        "            i=jnp.zeros((), jnp.uint32))\n",
        "      cache_entry = init_fn\n",
        "    else:\n",
        "      cache_entry = scope.get_variable('cache', 'entry')\n",
        "      if not isinstance(cache_entry, CacheEntry):\n",
        "        raise ValueError('Cache is not initialized.')\n",
        "\n",
        "      expected_shape = list(cache_entry.key.shape[:-2])\n",
        "      for attn_dim in attention_axis:\n",
        "        expected_shape[attn_dim] = 1\n",
        "      expected_shape = tuple(expected_shape) + inputs_q.shape[-1:]\n",
        "      if expected_shape != inputs_q.shape:\n",
        "        raise ValueError('Invalid shape provided, '\n",
        "                          'expected shape %s instead got %s.' %\n",
        "                          (expected_shape, inputs_q.shape))\n",
        "\n",
        "      cshape = cache_entry.key.shape\n",
        "      indices = [0] * len(cshape)\n",
        "      i = cache_entry.i\n",
        "      attn_size = onp.prod(onp.take(cshape, attention_axis))\n",
        "      for attn_dim in attention_axis:\n",
        "        attn_size //= cshape[attn_dim]\n",
        "        indices[attn_dim] = i // attn_size\n",
        "        i = i % attn_size\n",
        "\n",
        "      key = lax.dynamic_update_slice(cache_entry.key, key, indices)\n",
        "      value = lax.dynamic_update_slice(cache_entry.value, value, indices)\n",
        "      one = jnp.array(1, jnp.uint32)\n",
        "      cache_entry = cache_entry.replace(i=cache_entry.i + one,\n",
        "                                        key=key,\n",
        "                                        value=value)\n",
        "      \n",
        "\n",
        "      # TODO(levskaya): verify this is still needed in translation decoding.\n",
        "      key_padding_mask = jnp.broadcast_to(\n",
        "          (jnp.arange(cshape[1]) < cache_entry.i), cshape[:2])\n",
        "      key_padding_mask = key_padding_mask.astype(jnp.float32)[..., None]\n",
        "    scope.put_variable('cache', 'entry', cache_entry)\n",
        "\n",
        "  # create attention masks\n",
        "  mask_components = []\n",
        "\n",
        "  if causal_mask:\n",
        "    if cache and isinstance(cache_entry, CacheEntry):\n",
        "      bias_pre_shape = (1,) * (key.ndim - 1)\n",
        "      attn_shape = tuple(onp.take(key.shape, attention_axis))\n",
        "      attn_size = onp.prod(attn_shape)\n",
        "      ii = jnp.arange(attn_size, dtype=jnp.uint32)\n",
        "      mask = ii < cache_entry.i\n",
        "      mask_components.append(mask.reshape(bias_pre_shape + attn_shape))\n",
        "    else:\n",
        "      mask_components.append(nn.attention._make_causal_mask(key, attention_axis))\n",
        "\n",
        "  if padding_mask is not None:\n",
        "    if key_padding_mask is None:\n",
        "      key_padding_mask = padding_mask\n",
        "    padding_mask = nn.attention.make_padding_mask(\n",
        "        padding_mask_query=padding_mask,\n",
        "        padding_mask_key=key_padding_mask,\n",
        "        query_shape=query.shape,\n",
        "        key_shape=key.shape,\n",
        "        attention_axis=attention_axis)\n",
        "    mask_components.append(padding_mask)\n",
        "\n",
        "  if segmentation is not None:\n",
        "    if key_segmentation is None:\n",
        "      key_segmentation = segmentation\n",
        "    segmentation_mask = nn.attention.make_padding_mask(\n",
        "        padding_mask_query=segmentation,\n",
        "        padding_mask_key=key_segmentation,\n",
        "        query_shape=query.shape,\n",
        "        key_shape=key.shape,\n",
        "        attention_axis=attention_axis,\n",
        "        segmentation_mask=True)\n",
        "    mask_components.append(segmentation_mask)\n",
        "\n",
        "  if mask_components:\n",
        "    attention_mask = mask_components[0]\n",
        "    for component in mask_components[1:]:\n",
        "      attention_mask = jnp.logical_and(attention_mask, component)\n",
        "\n",
        "    # attention mask in the form of attention bias\n",
        "    attention_bias = lax.select(\n",
        "        attention_mask > 0, jnp.full(attention_mask.shape, 0.).astype(dtype),\n",
        "        jnp.full(attention_mask.shape, -1e10).astype(dtype))\n",
        "  else:\n",
        "    attention_bias = None\n",
        "\n",
        "  # apply attention\n",
        "  x = attention_fn(\n",
        "      query,\n",
        "      key,\n",
        "      value,\n",
        "      dtype=dtype,\n",
        "      axis=attention_axis,\n",
        "      bias=attention_bias,\n",
        "      precision=precision,\n",
        "      dropout_rng=dropout_rng,\n",
        "      dropout_rate=dropout_rate,\n",
        "      broadcast_dropout=broadcast_dropout,\n",
        "      deterministic=deterministic)\n",
        "\n",
        "  # back to the original inputs dimensions\n",
        "  out = scope.child(dense_general, name='out')(\n",
        "      x,\n",
        "      features=features,\n",
        "      axis=(-2, -1),\n",
        "      kernel_init=kernel_init,\n",
        "      bias_init=bias_init,\n",
        "      bias=bias,\n",
        "      dtype=dtype,\n",
        "      precision=precision)\n",
        "\n",
        "  return out\n",
        "\n",
        "x = jnp.ones((1, 2, 4))\n",
        "attn = functools.partial(multi_head_dot_product_attention, num_heads=1, cache=True, causal_mask=True)\n",
        "y, variables = init(attn)(random.PRNGKey(0), x, x)\n",
        "params = variables['param']\n",
        "cache = jax.tree_map(lambda fn: fn((1, 2)), variables['cache'])\n",
        "variables = variables.copy(cache=cache)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd3Smw99EWjC",
        "colab_type": "code",
        "outputId": "f010a3c2-f07d-4ef0-f1b0-21d44877ba05",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1590672864921,
          "user_tz": -120,
          "elapsed": 751,
          "user": {
            "displayName": "Jonathan Heek",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhqRcoo1w0woYaM99jSyWQaD-qfmHmeDpXHzHZd=s64",
            "userId": "00491914421152177709"
          }
        },
        "colab": {
          "height": 476
        }
      },
      "source": [
        "apply(attn, mutable='cache')(variables, x[:, 0:1], x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTFjZbRmlqZh",
        "colab_type": "code",
        "outputId": "5790b763-df4f-47c8-9f4e-53fd1e1eb1fd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1590672865722,
          "user_tz": -120,
          "elapsed": 526,
          "user": {
            "displayName": "Jonathan Heek",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhqRcoo1w0woYaM99jSyWQaD-qfmHmeDpXHzHZd=s64",
            "userId": "00491914421152177709"
          }
        },
        "colab": {
          "height": 85
        }
      },
      "source": [
        "@struct.dataclass\n",
        "class Embedding:\n",
        "  table: onp.ndarray\n",
        "\n",
        "  def lookup(self, indices):\n",
        "    return self.table[indices]\n",
        "\n",
        "  def attend(self, query):\n",
        "    return jnp.dot(query, self.table.T)\n",
        "\n",
        "# all the embedding module does is provide a convenient initializers\n",
        "\n",
        "def embedding(scope: Scope, num_embeddings: int, features: int, init_fn=nn.linear.default_embed_init) -> Embedding:\n",
        "  table = scope.param('table', init_fn, (num_embeddings, features))\n",
        "  return Embedding(table)\n",
        "\n",
        "embedding, _ = init(embedding)(random.PRNGKey(0), num_embeddings=2, features=3)\n",
        "print(embedding.table)\n",
        "print(embedding.lookup(1))\n",
        "print(embedding.attend(jnp.ones((1, 3,))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMlae0hem0u5",
        "colab_type": "code",
        "outputId": "dd9c5079-10e7-4944-e09a-e9f65573a733",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1590673618925,
          "user_tz": -120,
          "elapsed": 342,
          "user": {
            "displayName": "Jonathan Heek",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhqRcoo1w0woYaM99jSyWQaD-qfmHmeDpXHzHZd=s64",
            "userId": "00491914421152177709"
          }
        },
        "colab": {
          "height": 71
        }
      },
      "source": [
        "def lstm(scope, carry, inputs,\n",
        "         gate_fn=nn.activation.sigmoid, activation_fn=nn.activation.tanh,\n",
        "         kernel_init=nn.linear.default_kernel_init,\n",
        "         recurrent_kernel_init=nn.initializers.orthogonal(),\n",
        "         bias_init=nn.initializers.zeros):\n",
        "  r\"\"\"A long short-term memory (LSTM) cell.\n",
        "\n",
        "  the mathematical definition of the cell is as follows\n",
        "  .. math::\n",
        "      \\begin{array}{ll}\n",
        "      i = \\sigma(W_{ii} x + W_{hi} h + b_{hi}) \\\\\n",
        "      f = \\sigma(W_{if} x + W_{hf} h + b_{hf}) \\\\\n",
        "      g = \\tanh(W_{ig} x + W_{hg} h + b_{hg}) \\\\\n",
        "      o = \\sigma(W_{io} x + W_{ho} h + b_{ho}) \\\\\n",
        "      c' = f * c + i * g \\\\\n",
        "      h' = o * \\tanh(c') \\\\\n",
        "      \\end{array}\n",
        "  where x is the input, h is the output of the previous time step, and c is\n",
        "  the memory.\n",
        "\n",
        "  Args:\n",
        "    carry: the hidden state of the LSTM cell,\n",
        "      initialized using `LSTMCell.initialize_carry`.\n",
        "    inputs: an ndarray with the input for the current time step.\n",
        "      All dimensions except the final are considered batch dimensions.\n",
        "    gate_fn: activation function used for gates (default: sigmoid)\n",
        "    activation_fn: activation function used for output and memory update\n",
        "      (default: tanh).\n",
        "    kernel_init: initializer function for the kernels that transform\n",
        "      the input (default: lecun_normal).\n",
        "    recurrent_kernel_init: initializer function for the kernels that transform\n",
        "      the hidden state (default: orthogonal).\n",
        "    bias_init: initializer for the bias parameters (default: zeros)\n",
        "  Returns:\n",
        "    A tuple with the new carry and the output.\n",
        "  \"\"\"\n",
        "  c, h = carry\n",
        "  hidden_features = h.shape[-1]\n",
        "  # input and recurrent layers are summed so only one needs a bias.\n",
        "  dense_h = lambda name: scope.child(dense, name)(\n",
        "      h, features=hidden_features, bias=True,\n",
        "      kernel_init=recurrent_kernel_init, bias_init=bias_init)\n",
        "  dense_i = lambda name: scope.child(dense, name)(\n",
        "      inputs, features=hidden_features, bias=False,\n",
        "      kernel_init=kernel_init)\n",
        "  i = gate_fn(dense_i(name='ii') + dense_h(name='hi'))\n",
        "  f = gate_fn(dense_i(name='if') + dense_h(name='hf'))\n",
        "  g = activation_fn(dense_i(name='ig') + dense_h(name='hg'))\n",
        "  o = gate_fn(dense_i(name='io') + dense_h(name='ho'))\n",
        "  new_c = f * c + i * g\n",
        "  new_h = o * activation_fn(new_c)\n",
        "  return (new_c, new_h), new_h\n",
        "\n",
        "def lstm_init_carry(batch_dims, size, init_fn=jnp.zeros):\n",
        "  shape = batch_dims + (size,)\n",
        "  return init_fn(shape), init_fn(shape)\n",
        "\n",
        "x = jnp.ones((1, 2))\n",
        "carry = lstm_init_carry((1,), 3)\n",
        "y, variables = init(lstm)(random.PRNGKey(0), carry, x)\n",
        "jax.tree_map(onp.shape, (y, variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "flax functional engine.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      }
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}