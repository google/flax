{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why NNX?\n",
    "\n",
    "<!-- open in colab button -->\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/flax/blob/main/flax/experimental/nnx/docs/why.ipynb)\n",
    "\n",
    "Four years ago we developed the Flax \"Linen\" API to support modeling research on JAX, with a focus on scaling scaling and performance.  We've learned a lot from our users over these years.\n",
    "\n",
    "We introduced some ideas that have proven to be good:\n",
    " - Organizing variables into [collections](https://flax.readthedocs.io/en/latest/glossary.html#term-Variable-collections) or types to support JAX transforms and segregation of different data types in training loops.\n",
    " - Automatic and efficient [PRNG management](https://flax.readthedocs.io/en/latest/glossary.html#term-RNG-sequences) (with support for splitting/broadcast control across map transforms)\n",
    " - [Variable Metadata](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/_autosummary/flax.linen.with_partitioning.html#flax.linen.with_partitioning) for SPMD annotations, optimizer metadata, and other uses.\n",
    "\n",
    "However, one choice we made was to use functional \"define by call\" semantics for NN programming via the lazy initialization of parameters.  This made for concise (`compact`) implementation code, allowed for a single specification when transforming a layer, and aligned our API with  Haiku.  Lazy initialization meant that the semantics of modules and variables in Flax were non-pythonic and often surprising.  It also led to implementation complexity and obscured the core ideas of transformations on neural nets.\n",
    "\n",
    "NNX is an attempt to keep the features that made Linen useful while introducing some new principles:\n",
    "\n",
    "- Regular Python semantics for Modules, including (within JIT boundaries) support for mutability and shared references.\n",
    "- A simple API to interact directly with the JAX, this includes the ability to easily implement custom lifted Modules and other purely functional tricks.\n",
    "\n",
    "We'd love to hear from any of our users about their thoughts on these ideas.\n",
    "\n",
    "[[nnx on github](https://github.com/google/flax/tree/main/flax/experimental/nnx)]\n",
    "[[this doc on github](https://github.com/google/flax/blob/main/flax/experimental/nnx/docs/why.ipynb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U git+https://github.com/google/flax.git\n",
    "from functools import partial\n",
    "import jax\n",
    "from jax import random, numpy as jnp\n",
    "from flax.experimental import nnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNX is Pythonic\n",
    "The main feature of NNX Module is that it adheres to Python semantics. This means that:\n",
    "\n",
    "* fields are mutable so you can perform inplace updates\n",
    "* Module references can be shared between multiple Modules\n",
    "* Module construction implies parameter initialization\n",
    "* Module methods can be called directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "outputId": "d8ef66d5-6866-4d5c-94c2-d22512bfe718"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model = CounterLinear(\n",
      "  linear=Linear(\n",
      "    in_features=4,\n",
      "    out_features=4,\n",
      "    use_bias=True,\n",
      "    dtype=None,\n",
      "    param_dtype=<class 'jax.numpy.float32'>,\n",
      "    precision=None,\n",
      "    kernel_init=<function variance_scaling.<locals>.init at 0x7f3dc9ad3370>,\n",
      "    bias_init=<function zeros at 0x7f3e04846e60>,\n",
      "    dot_general=<function dot_general at 0x7f3e06edd2d0>\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Count(nnx.Variable):   # custom Variable types define the \"collections\"\n",
    "  pass\n",
    "\n",
    "\n",
    "class CounterLinear(nnx.Module):\n",
    "  def __init__(self, din, dout, *, rngs): # explicit RNG threading\n",
    "    self.linear = nnx.Linear(din, dout, rngs=rngs)\n",
    "    self.count = Count(jnp.zeros((), jnp.int32)) # typed Variable collections\n",
    "\n",
    "  def __call__(self, x):\n",
    "    self.count.value += 1  # in-place stateful updates\n",
    "    return self.linear(x)\n",
    "\n",
    "\n",
    "model = CounterLinear(4, 4, rngs=nnx.Rngs(0))  # no special `init` method\n",
    "y = model(jnp.ones((2, 4)))  # call methods directly\n",
    "\n",
    "print(f'{model = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because NNX Modules contain their own state, they are very easily to inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "outputId": "10a46b0f-2993-4677-c26d-36a4ddf33449"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.count = Array(1, dtype=int32)\n",
      "model.linear.kernel = Array([[ 0.4541089 , -0.5264876 , -0.36505195, -0.57566494],\n",
      "       [ 0.38802508,  0.5655534 ,  0.4870657 ,  0.2267774 ],\n",
      "       [-0.9015767 ,  0.24465278, -0.5844707 ,  0.18421966],\n",
      "       [-0.06992685, -0.64693886,  0.20232596,  1.1200062 ]],      dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(f'{model.count = }')\n",
    "print(f'{model.linear.kernel = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intuitive Surgery\n",
    "\n",
    "In NNX surgery can be done at the Module level by simply updating / replacing existing fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "outputId": "e6f86be8-3537-4c48-f471-316ee0fb6c45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[1.7531997, 1.6318591, 2.1417565, 3.120555 ],\n",
       "       [1.7531997, 1.6318591, 2.1417565, 3.120555 ]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretend this came from a checkpoint or elsewhere:\n",
    "pretrained_weight = random.uniform(random.key(0), (4, 4))\n",
    "\n",
    "# you can replace weights directly\n",
    "model.linear.kernel = pretrained_weight\n",
    "y = model(jnp.ones((2, 4)))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "outputId": "5190ac7b-12f7-4400-d5bb-f91b97a557b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[1.624419  , 0.8313738 , 0.37612876, 1.9937458 ],\n",
       "       [1.624419  , 0.8313738 , 0.37612876, 1.9937458 ]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_pretrained_fragment():\n",
    "  # pretend this inits / loads some fragment of a model\n",
    "  replacement = nnx.Linear(4, 4, rngs=nnx.Rngs(1))\n",
    "  return replacement\n",
    "\n",
    "# you can replace modules directly\n",
    "model.linear = load_pretrained_fragment()\n",
    "y = model(jnp.ones((2, 4)))\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only is this easier than messing with dictionary structures and aligning that with code changes, but one can even replace a field with a completely different Module type, or even change the architecture (e.g. share two Modules that were not shared before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rngs = nnx.Rngs(0)\n",
    "model = nnx.Sequence(\n",
    "  [\n",
    "    nnx.Conv(1, 16, [3, 3], padding='SAME', rngs=rngs),\n",
    "    partial(nnx.max_pool, window_shape=(2, 2), strides=(2, 2)),\n",
    "    nnx.Conv(16, 32, [3, 3], padding='SAME', rngs=rngs),\n",
    "    partial(nnx.max_pool, window_shape=(2, 2), strides=(2, 2)),\n",
    "    lambda x: x.reshape((x.shape[0], -1)),  # flatten\n",
    "    nnx.Linear(32 * 7 * 7, 10, rngs=rngs),\n",
    "  ]\n",
    ")\n",
    "\n",
    "y = model(jnp.ones((2, 28, 28, 1)))\n",
    "\n",
    "# Do some weird surgery of the stack:\n",
    "for i, layer in enumerate(model):\n",
    "  if isinstance(layer, nnx.Conv):\n",
    "    model[i] = nnx.Linear(layer.in_features, layer.out_features, rngs=rngs)\n",
    "\n",
    "y = model(jnp.ones((2, 28, 28, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here we are replacing `Conv` with `Linear` as a silly example, but in reality you would do things like replacing a layer with its quantized version, or changing a layer with an optimized version, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting with JAX is easy\n",
    "\n",
    "While NNX Modules inherently follow reference semantics, they can be easily converted into a pure functional representation that can be used with JAX transformations and other value-based, functional code.\n",
    "\n",
    "NNX has two very simple APIs to interact with JAX: `split` and `merge`.\n",
    "\n",
    "The `Module.split` method allows you to convert into a `State` dict-like object that contains the dynamic state of the Module, and a `GraphDef` object that contains the static structure of the Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "outputId": "9a3f378b-739e-4f45-9968-574651200ede"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state = State({\n",
      "  'count': Array(0, dtype=int32),\n",
      "  'linear/bias': Array([0., 0., 0., 0.], dtype=float32),\n",
      "  'linear/kernel': Array([[ 0.4541089 , -0.5264876 , -0.36505195, -0.57566494],\n",
      "         [ 0.38802508,  0.5655534 ,  0.4870657 ,  0.2267774 ],\n",
      "         [-0.9015767 ,  0.24465278, -0.5844707 ,  0.18421966],\n",
      "         [-0.06992685, -0.64693886,  0.20232596,  1.1200062 ]],      dtype=float32)\n",
      "})\n",
      "\n",
      "static = GraphDef(\n",
      "  type=CounterLinear,\n",
      "  index=0,\n",
      "  static_fields=(),\n",
      "  variables=(('count', Count(\n",
      "      value=Empty\n",
      "    )),),\n",
      "  submodules=(\n",
      "    ('linear', GraphDef(\n",
      "      type=Linear,\n",
      "      index=1,\n",
      "      static_fields=(('bias_init', <function zeros at 0x7f3e04846e60>), ('dot_general', <function dot_general at 0x7f3e06edd2d0>), ('dtype', None), ('in_features', 4), ('kernel_init', <function variance_scaling.<locals>.init at 0x7f3dc9ad3370>), ('out_features', 4), ('param_dtype', <class 'jax.numpy.float32'>), ('precision', None), ('use_bias', True)),\n",
      "      variables=(('bias', Param(\n",
      "              value=Empty\n",
      "            )), ('kernel', Param(\n",
      "              value=Empty\n",
      "            ))),\n",
      "      submodules=()\n",
      "    ))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CounterLinear(4, 4, rngs=nnx.Rngs(0))\n",
    "\n",
    "state, static = model.split()\n",
    "\n",
    "# state is a dictionary-like JAX pytree\n",
    "print(f'{state = }')\n",
    "\n",
    "# static is also a JAX pytree, but containing no data, just metadata\n",
    "print(f'\\n{static = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GraphDef.merge` method allows you to take a `GraphDef` and one or more `State` objects and merge them back into a `Module` object.\n",
    "\n",
    "Using `split` and `merge` in conjunction allows you to carry your Module in and out of any JAX transformation. Here is a simple jitted `forward` function as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "outputId": "0007d357-152a-449e-bcb9-b1b5a91d2d8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.shape = (2, 4)\n",
      "state[\"count\"] = Array(1, dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def forward(static: nnx.GraphDef, state: nnx.State, x: jax.Array):\n",
    "  model = static.merge(state)\n",
    "  y = model(x)\n",
    "  state, _ = model.split()\n",
    "  return y, state\n",
    "\n",
    "x = jnp.ones((2, 4))\n",
    "y, state = forward(static,state, x)\n",
    "\n",
    "print(f'{y.shape = }')\n",
    "print(f'{state[\"count\"] = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom lifting and transformation\n",
    "\n",
    "By using the same mechanism inside Module methods you can implement lifted Modules, that is, Modules that use a JAX transformation to have a distinct behavior.\n",
    "\n",
    "One of Linen's current pain points is that it is not easy to interact with JAX transformations that are not currently supported by the framework. NNX makes it very easy to implement custom lifted Modules or bespoke custom functional transforms for specific use cases.\n",
    "\n",
    "As an example here we will create a `LinearEnsemble` Module that uses `jax.vmap` both during `__init__` and `__call__` to vectorize the computation over multiple `CounterLinear` models (defined above). The example is a little bit longer, but notice how each method conceptually very simple.\n",
    "\n",
    "It uses the single additional method `update` to locally modify model state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "outputId": "fdd212d7-4994-4fa5-d922-5a7d7cfad3e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.shape = (8, 4)\n",
      "ensemble.models.count = Array(1, dtype=int32)\n",
      "state = State({\n",
      "  'models/count': (),\n",
      "  'models/linear/bias': (8, 4),\n",
      "  'models/linear/kernel': (8, 4, 4)\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "class LinearEnsemble(nnx.Module):\n",
    "  def __init__(self, din, dout, *, num_models, rngs: nnx.Rngs):\n",
    "    # get raw rng seeds\n",
    "    keys = rngs.fork(num_models) # split all keys into `num_models`\n",
    "\n",
    "    # define pure init fn and vmap\n",
    "    def vmap_init(keys):\n",
    "      return CounterLinear(din, dout, rngs=nnx.Rngs(keys)).split(\n",
    "        nnx.Param, Count\n",
    "      )\n",
    "    params, counts, static = jax.vmap(\n",
    "      vmap_init, in_axes=(0,), out_axes=(0, None, None)\n",
    "    )(keys)\n",
    "\n",
    "    # update wrapped submodule reference\n",
    "    self.models = static.merge(params, counts)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # get module values, define pure fn,\n",
    "    # notice that we split the data into two collections by their types.\n",
    "    params, counts, static = self.models.split(nnx.Param, Count)\n",
    "\n",
    "    # define pure init fn and vmap\n",
    "    def vmap_apply(x, params, counts, static):\n",
    "      model = static.merge(params, counts)\n",
    "      y = model(x)\n",
    "      params, counts, static = model.split(nnx.Param, Count)\n",
    "      return y, params, counts, static\n",
    "\n",
    "    y, params, counts, static = jax.vmap(\n",
    "        vmap_apply,\n",
    "        in_axes=(None, 0, None, None),\n",
    "        out_axes=(0, 0, None, None)\n",
    "    )(x, params, counts, static)\n",
    "\n",
    "    # update wrapped module\n",
    "    # uses `update` to integrate the new state\n",
    "    self.models.update(params, counts, static)\n",
    "    return y\n",
    "\n",
    "x = jnp.ones((4,))\n",
    "ensemble = LinearEnsemble(4, 4, num_models=8, rngs=nnx.Rngs(0))\n",
    "\n",
    "# forward pass\n",
    "y = ensemble(x)\n",
    "\n",
    "print(f'{y.shape = }')\n",
    "print(f'{ensemble.models.count = }')\n",
    "print(f'state = {jax.tree_map(jnp.shape, ensemble.get_state())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convenience lifted transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like linen, for convenience we still provide simple lifted transforms for standard JAX transforms, usable as class transforms and decorators.  We've endeavored to simplify the API for scan and vmap compared to the flax specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "outputId": "c4800a49-efd1-4ee5-e703-6e63e18da4cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State({\n",
       "  'scan_module/bias': Array([[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]], dtype=float32),\n",
       "  'scan_module/kernel': Array([[[-0.32325608,  0.16164146],\n",
       "          [ 0.46505648, -0.34060344]],\n",
       "  \n",
       "         [[-1.1558908 ,  1.2445341 ],\n",
       "          [-1.3710847 , -0.1787171 ]],\n",
       "  \n",
       "         [[-0.68510336,  0.25847596],\n",
       "          [ 1.0730107 , -0.11857361]],\n",
       "  \n",
       "         [[-0.01770882,  0.5472832 ],\n",
       "          [-0.84826714,  0.17867221]]], dtype=float32)\n",
       "})"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class transform:\n",
    "ScannedLinear = nnx.Scan(nnx.Linear, variable_axes={nnx.Param: 0}, length=4)\n",
    "\n",
    "scanned = ScannedLinear(2, 2, rngs=nnx.Rngs(0))\n",
    "scanned.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "outputId": "9efd6e71-d180-4674-ade0-2b02057a400b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State({\n",
       "  'model/bias': Array([[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]], dtype=float32),\n",
       "  'model/kernel': Array([[[-0.32325608,  0.16164146],\n",
       "          [ 0.46505648, -0.34060344]],\n",
       "  \n",
       "         [[-1.1558908 ,  1.2445341 ],\n",
       "          [-1.3710847 , -0.1787171 ]],\n",
       "  \n",
       "         [[-0.68510336,  0.25847596],\n",
       "          [ 1.0730107 , -0.11857361]],\n",
       "  \n",
       "         [[-0.01770882,  0.5472832 ],\n",
       "          [-0.84826714,  0.17867221]]], dtype=float32)\n",
       "})"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# method decorators:\n",
    "\n",
    "class ScannedLinear(nnx.Module):\n",
    "\n",
    "  @partial(nnx.scan, variable_axes={nnx.Param: 0}, length=4)\n",
    "  def __init__(self, din, dout, *, rngs: nnx.Rngs):\n",
    "    self.model = nnx.Linear(din, dout, rngs=nnx.Rngs(rngs))\n",
    "\n",
    "  @partial(nnx.scan, variable_axes={nnx.Param: 0}, length=4)\n",
    "  def __call__(self, x):\n",
    "    return self.model(x)\n",
    "\n",
    "scanned = ScannedLinear(2, 2, rngs=nnx.Rngs(0))\n",
    "scanned.get_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside: Why aren't Modules Pytrees?\n",
    "\n",
    "A common questions is why aren't NNX Modules registered as Pytrees? (in the style of Equinox, Treex, PytreeClass, etc.)  It _is_ trivial to define a pytree registration in terms of `split`/`merge`.\n",
    "\n",
    "The problem is that Pytrees impose value semantics (referencial transparency) while Modules assume reference semantics, and therefore it is dangerous in general to automatically treat Modules as Pytrees.\n",
    "\n",
    "As an example, lets take a look at what would happen if we allowed this very simple program to be valid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def f(m1: nnx.Module, m2: nnx.Module):\n",
    "  return m1, m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are just creating a jitted function `f` that takes in two Modules `(m1, m2)` and returns them as is. What could go wrong?\n",
    "\n",
    "There are two main problems with this:\n",
    "* Shared references are not maintained, that is, if `m1.shared` is the same as `m2.shared` outside `f`, this will NOT be true both inside `f`, and at the output of `f`.\n",
    "* Even if you accept this fact and added code to compensate for this, `f` would now behave differently depending on whether its being `jit`ted or not, this is an undesirable asymmetry and `jit` would no longer be a no-op."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardized \"Hooks\"\n",
    "\n",
    "NNX introduces a standard getter/setter/creator interface for custom variables (similar to Haiku hooks).  This is used internally to support SPMD metadata for managing sharding information, but is available for user-defined applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "outputId": "c4e6586a-bfe0-4f26-d05b-8c9e395971b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.kernel.shape = (4, 8)\n",
      "outer kernel shape = (8, 4)\n"
     ]
    }
   ],
   "source": [
    "class TransposedParam(nnx.Variable):\n",
    "  def create_value(self, value):\n",
    "    return value.T  # called on variable creation to transform initial value\n",
    "  def get_value(self):\n",
    "    return self.value.T  # called when value fetched via module getattr\n",
    "  def set_value(self, value):\n",
    "    return self.replace(value=value.T)  # called when setting value from module setattr\n",
    "\n",
    "\n",
    "class OddLinear(nnx.Module):\n",
    "  def __init__(self, din, dout, *, rngs):\n",
    "    self.kernel = TransposedParam(random.uniform(rngs.params(), (din, dout)))\n",
    "    self.bias = nnx.Param(jnp.zeros((dout,)))\n",
    "\n",
    "  def __call__(self, x):\n",
    "    print(f'{self.kernel.shape = }')\n",
    "    return x @ self.kernel + self.bias\n",
    "\n",
    "\n",
    "model = OddLinear(4, 8, rngs=nnx.Rngs(0))\n",
    "y = model(jnp.ones((2, 4)))\n",
    "\n",
    "print(f'outer kernel shape = {model.split()[0][\"kernel\"].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPMD metadata is handled using `nnx.with_partitioning` helpers, but it's easy to add one's own metadata schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "outputId": "ef312738-0f56-4c0e-9aaf-3319d131f1a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state.variables['kernel'].meta='foo'\n",
      "state.variables['kernel'].other_meta=0\n",
      "state.variables['bias'].meta='bar'\n",
      "state.variables['bias'].other_meta=1\n"
     ]
    }
   ],
   "source": [
    "class MetadataParam(nnx.Param):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    for key in kwargs:\n",
    "      setattr(self, key, kwargs[key])\n",
    "    super().__init__(*args)\n",
    "\n",
    "\n",
    "class AnnotatedLinear(nnx.Module):\n",
    "  def __init__(self, din, dout, *, rngs):\n",
    "    self.kernel = TransposedParam(random.uniform(rngs.params(), (din, dout)), meta='foo', other_meta=0)\n",
    "    self.bias = TransposedParam(jnp.zeros((dout,)), meta='bar', other_meta=1)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return x @ self.kernel + self.bias\n",
    "\n",
    "\n",
    "model = AnnotatedLinear(4, 8, rngs=nnx.Rngs(0))\n",
    "y = model(jnp.ones((2, 4)))\n",
    "\n",
    "state, static = model.split()\n",
    "\n",
    "print(f\"{state.variables['kernel'].meta=}\\n{state.variables['kernel'].other_meta=}\")\n",
    "print(f\"{state.variables['bias'].meta=}\\n{state.variables['bias'].other_meta=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape Inference\n",
    "\n",
    "Shape inference is still possible in NNX using abstract evaluation when it's really needed, it just isn't automatic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "outputId": "942a3788-bcbf-426d-87e6-c5a041172c64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State({\n",
       "  'encoder/bias': (4,),\n",
       "  'encoder/kernel': (3, 3, 3, 4),\n",
       "  'linear/bias': (4,),\n",
       "  'linear/kernel': (144, 4)\n",
       "})"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batched_flatten(x):\n",
    "  return jnp.reshape(x, (x.shape[0], -1))\n",
    "\n",
    "class Example(nnx.Module):\n",
    "  def __init__(self, *,\n",
    "               in_filters=3,\n",
    "               out_filters=4,\n",
    "               input_shape=None,  # provide an example input size\n",
    "               rngs):\n",
    "      self.encoder = nnx.Conv(in_filters, out_filters,\n",
    "                              kernel_size=(3, 3),\n",
    "                              strides=(1, 1),\n",
    "                              padding=\"SAME\",\n",
    "                              rngs=rngs)\n",
    "      # calculate the flattened shape post-conv using jax.eval_shape\n",
    "      encoded_shape = jax.eval_shape(\n",
    "          lambda x: batched_flatten(self.encoder(x)),\n",
    "          jax.ShapeDtypeStruct(input_shape, jnp.float32)\n",
    "      ).shape\n",
    "      # use this shape information to continue initializing\n",
    "      self.linear = nnx.Linear(encoded_shape[-1], 4, rngs=rngs)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    x = self.encoder(x)\n",
    "    x = batched_flatten(x)\n",
    "    return self.linear(x)\n",
    "\n",
    "model = Example(in_filters=3,\n",
    "                out_filters=4,\n",
    "                input_shape=(2, 6, 6, 3),\n",
    "                rngs=nnx.Rngs(0))\n",
    "\n",
    "state, static = model.split()\n",
    "jax.tree_map(jnp.shape, state)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
