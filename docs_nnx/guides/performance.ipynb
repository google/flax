{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Considerations\n",
    "Currently `nnx.jit` traverses the object graph in Python. This is slow and primarily affects the small model regime, as the Python overhead starts to disappear as the model's width grows. To solve this in general, we will be developing a Rust extension called `flaxlib` (see first steps in #4196) to speedup some of the traversal logic in `graph.py`, similar to how JAX solved the same issue with `jaxlib` for standard pytrees. \n",
    "\n",
    "Meanwhile, there is a pattern you can use to remove the python overhead using regular `jax.jit` + `nnx.split` / `nnx.merge` to stage out the traversal logic. Take this code that uses `nnx.jit` as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "class Model(nnx.Module):\n",
    "  def __init__(self, din, dmid, dout, rngs: nnx.Rngs):\n",
    "    self.linear = nnx.Linear(din, dmid, rngs=rngs)\n",
    "    self.bn = nnx.BatchNorm(dmid, rngs=rngs)\n",
    "    self.dropout = nnx.Dropout(0.2, rngs=rngs)\n",
    "    self.linear_out = nnx.Linear(dmid, dout, rngs=rngs)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    x = nnx.relu(self.dropout(self.bn(self.linear(x))))\n",
    "    return self.linear_out(x)\n",
    "\n",
    "model = Model(2, 64, 3, rngs=nnx.Rngs(0))  # eager initialization\n",
    "optimizer = nnx.Optimizer(model, optax.adam(1e-3))  # reference sharing\n",
    "metrics = nnx.MultiMetric(\n",
    "  loss=nnx.metrics.Average('loss'),\n",
    ")\n",
    "\n",
    "@nnx.jit  # <== currently slow\n",
    "def train_step(model, optimizer, metrics, x, y):\n",
    "  def loss_fn(model):\n",
    "    y_pred = model(x)  # call methods directly\n",
    "    return ((y_pred - y) ** 2).mean()\n",
    "\n",
    "  loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "  optimizer.update(grads)  # in-place updates\n",
    "  metrics.update(loss=loss)\n",
    "\n",
    "  return loss\n",
    "  \n",
    "for _ in range(10):\n",
    "  x, y = jnp.ones((32, 2)), jnp.zeros((32, 3))\n",
    "  loss = train_step(model, optimizer, metrics, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed it up you can use `nnx.split` before starting the training loop to create a `graphdef` and `state` for the NNX objects which are fast to traverse, and then call `merge` + `split` inside the `jax.jit`-decorated function so they only run once during tracing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(2, 64, 3, rngs=nnx.Rngs(0))  # eager initialization\n",
    "optimizer = nnx.Optimizer(model, optax.adamw(1e-3))  # reference sharing\n",
    "metrics = nnx.MultiMetric(\n",
    "  loss=nnx.metrics.Average('loss'),\n",
    ")\n",
    "# split before training loop\n",
    "graphdef, state = nnx.split((model, optimizer, metrics))\n",
    "\n",
    "@jax.jit  # regular JAX\n",
    "def train_step(graphdef, state, x, y):\n",
    "  # merge at the beginning of the function\n",
    "  model, optimizer, metrics = nnx.merge(graphdef, state)\n",
    "\n",
    "  def loss_fn(model):\n",
    "    y_pred = model(x)  # call methods directly\n",
    "    return ((y_pred - y) ** 2).mean()\n",
    "\n",
    "  loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "  optimizer.update(grads)\n",
    "  metrics.update(loss=loss)\n",
    "\n",
    "  # split at the end of the function\n",
    "  _, state = nnx.split((model, optimizer, metrics))\n",
    "\n",
    "  # return new state\n",
    "  return state, loss\n",
    "\n",
    "for _ in range(10):\n",
    "  x, y = jnp.ones((32, 2)), jnp.zeros((32, 3))\n",
    "  state, loss = train_step(graphdef, state, x, y)\n",
    "\n",
    "# update objects after training\n",
    "nnx.update((model, optimizer, metrics), state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training loop is done (or whenever need) `nnx.update` can be used to update `model`, `optimizer`, and `metrics` to a new `state`."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
