{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model surgery\n",
    "\n",
    "Model surgery is an act of making modifications on an existing neural network's building blocks and parameters, such as layer replacement, parameter or state manipulation, or even \"monkey patching\". In this guide, you will learn how to perform model surgery in Flax NNX using several real-world scenarios:\n",
    "\n",
    "* __Pythonic `nnx.Module` manipulation__: Using Pythonic ways to manipulate sub-`Module`s given a model.\n",
    "\n",
    "* __Manipulation of an abstract model or state__: A key trick for playing with `flax.nnx.Module`s and states without memory allocation.\n",
    "\n",
    "* __Checkpoint surgery from a raw state to model__: How to manipulate parameter states when they are incompatible with existing model code.\n",
    "\n",
    "* __Partial initialization__: How to initialize only a part of the model from scratch using a naive method or a memory-efficient method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from pprint import pprint\n",
    "import functools\n",
    "\n",
    "import jax\n",
    "from jax import lax, numpy as jnp, tree_util as jtu\n",
    "\n",
    "from jax.sharding import PartitionSpec, Mesh, NamedSharding\n",
    "from jax.experimental import mesh_utils\n",
    "import flax\n",
    "from flax import nnx\n",
    "import flax.traverse_util\n",
    "import numpy as np\n",
    "import orbax.checkpoint as orbax\n",
    "\n",
    "key = jax.random.key(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerMLP(nnx.Module):\n",
    "  def __init__(self, dim, rngs: nnx.Rngs):\n",
    "    self.linear1 = nnx.Linear(dim, dim, rngs=rngs)\n",
    "    self.linear2 = nnx.Linear(dim, dim, rngs=rngs)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    x = self.linear1(x)\n",
    "    return self.linear2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pythonic `nnx.Module` manipulation\n",
    "\n",
    "It is easier to perform model surgery when:\n",
    "\n",
    "1) You already have a fully fleshed-out model loaded with correct parameters; and\n",
    "2) You don't intend to change your model definition code.\n",
    "\n",
    "You can perform a variety of Pythonic operations on its sub-`Module`s, such as sub-`Module` swapping, `Module` sharing, variable sharing, and monkey-patching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerMLP(4, rngs=nnx.Rngs(0))\n",
    "x = jax.random.normal(jax.random.key(42), (3, 4))\n",
    "np.testing.assert_allclose(model(x), model.linear2(model.linear1(x)))\n",
    "\n",
    "# Sub-`Module` swapping.\n",
    "original1, original2 = model.linear1, model.linear2\n",
    "model.linear1, model.linear2 = model.linear2, model.linear1\n",
    "np.testing.assert_allclose(model(x), original1(original2(x)))\n",
    "\n",
    "# `Module` sharing (tying all weights together).\n",
    "model = TwoLayerMLP(4, rngs=nnx.Rngs(0))\n",
    "model.linear2 = model.linear1\n",
    "assert not hasattr(nnx.state(model), 'linear2')\n",
    "np.testing.assert_allclose(model(x), model.linear1(model.linear1(x)))\n",
    "\n",
    "# Variable sharing (weight-tying).\n",
    "model = TwoLayerMLP(4, rngs=nnx.Rngs(0))\n",
    "model.linear1.kernel = model.linear2.kernel  # the bias parameter is kept separate\n",
    "assert 'linear2' in nnx.state(model)\n",
    "assert 'bias' in nnx.state(model)['linear2']\n",
    "assert not hasattr(nnx.state(model)['linear2'], 'kernel')\n",
    "\n",
    "# Monkey-patching.\n",
    "model = TwoLayerMLP(4, rngs=nnx.Rngs(0))\n",
    "def awesome_layer(x): return x\n",
    "model.linear2 = awesome_layer\n",
    "np.testing.assert_allclose(model(x), model.linear1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an abstract model or state without memory allocation\n",
    "\n",
    "To do more complex model surgery, the key technique you can use is creating and manipulating an abstract model or state without allocating any real parameter data. This makes trial iteration faster and removes any concern on memory constraints.\n",
    "\n",
    "To create an abstract model:\n",
    "\n",
    "* Create a function that returns a valid Flax NNX model; and\n",
    "* Run `nnx.eval_shape` (not `jax.eval_shape`) upon it.\n",
    "\n",
    "Now you can use `nnx.split` as usual to get its abstract state. Note that all fields that should be `jax.Array`s in a real model are now of an abstract `jax.ShapeDtypeStruct` type with only shape/dtype/sharding information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State({\n",
      "  'linear1': {\n",
      "    'bias': VariableState(\n",
      "      type=Param,\n",
      "      value=ShapeDtypeStruct(shape=(4,), dtype=float32)\n",
      "    ),\n",
      "    'kernel': VariableState(\n",
      "      type=Param,\n",
      "      value=ShapeDtypeStruct(shape=(4, 4), dtype=float32)\n",
      "    )\n",
      "  },\n",
      "  'linear2': {\n",
      "    'bias': VariableState(\n",
      "      type=Param,\n",
      "      value=ShapeDtypeStruct(shape=(4,), dtype=float32)\n",
      "    ),\n",
      "    'kernel': VariableState(\n",
      "      type=Param,\n",
      "      value=ShapeDtypeStruct(shape=(4, 4), dtype=float32)\n",
      "    )\n",
      "  }\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "abs_model = nnx.eval_shape(lambda: TwoLayerMLP(4, rngs=nnx.Rngs(0)))\n",
    "gdef, abs_state = nnx.split(abs_model)\n",
    "pprint(abs_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you fill every `nnx.VariableState` pytree leaf's `value` attributes with real `jax.Array`s, the abstract model becomes equivalent to a real model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerMLP(4, rngs=nnx.Rngs(0))\n",
    "abs_state['linear1']['kernel'].value = model.linear1.kernel\n",
    "abs_state['linear1']['bias'].value = model.linear1.bias\n",
    "abs_state['linear2']['kernel'].value = model.linear2.kernel\n",
    "abs_state['linear2']['bias'].value = model.linear2.bias\n",
    "nnx.update(abs_model, abs_state)\n",
    "np.testing.assert_allclose(abs_model(x), model(x))  # They are equivalent now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint surgery\n",
    "\n",
    "With the abstract state technique in hand, you can perform arbitrary manipulation on any checkpoint - or runtime parameter pytree - to make them fit with your given model code, and then call `nnx.update` to merge them.\n",
    "\n",
    "This can be helpful if you are trying to significantly change the model code - for example, when migrating from Flax Linen to Flax NNX - and old weights are no longer naturally compatible.\n",
    "\n",
    "Let's run a simple example here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a version of model into a checkpoint\n",
    "checkpointer = orbax.PyTreeCheckpointer()\n",
    "old_model = TwoLayerMLP(4, rngs=nnx.Rngs(0))\n",
    "checkpointer.save(f'/tmp/nnx-surgery-state', nnx.state(model), force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this new model, the sub-`Module`s are renamed from `linear(1|2)` to `layer(1|2)`. Since the pytree structure has changed, it is impossible to directly load the old checkpoint with the new model state structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will throw error: <class 'ValueError'>: Dict key mismatch; expected keys: ['linear1', 'linear2']; dict: {'layer1': {'bias': {'value': RestoreArgs(restore_type=None, dtype=None)}, 'kernel': {'value': RestoreArgs(restore_type=None, dtype=None)}}, 'layer2': {'bias': {'value': RestoreArgs(restore_type=None, dtype=None)}, 'kernel': {'value': RestoreArgs(restore_type=None, dtype=None)}}}.\n"
     ]
    }
   ],
   "source": [
    "class ModifiedTwoLayerMLP(nnx.Module):\n",
    "  def __init__(self, dim, rngs: nnx.Rngs):\n",
    "    self.layer1 = nnx.Linear(dim, dim, rngs=rngs)  # no longer linear1!\n",
    "    self.layer2 = nnx.Linear(dim, dim, rngs=rngs)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    x = self.layer1(x)\n",
    "    return self.layer2(x)\n",
    "\n",
    "abs_model = nnx.eval_shape(lambda: ModifiedTwoLayerMLP(4, rngs=nnx.Rngs(0)))\n",
    "try:\n",
    "  with_item = checkpointer.restore('/tmp/nnx-surgery-state', item=nnx.state(abs_model))\n",
    "  print(with_item)\n",
    "except Exception as e:\n",
    "  print(f'This will throw error: {type(e)}: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you can load the parameter pytree as a raw dictionary, perform the renames, and generate a new state that is guaranteed to be compatible with your new model definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'linear1': {'bias': {'value': Array([0., 0., 0., 0.], dtype=float32)},\n",
      "             'kernel': {'value': Array([[-0.80345297, -0.34071913, -0.9408296 ,  0.01005968],\n",
      "       [ 0.26146442,  1.1247735 ,  0.54563737, -0.374164  ],\n",
      "       [ 1.0281805 , -0.6798804 , -0.1488401 ,  0.05694951],\n",
      "       [-0.44308168, -0.60587114,  0.434087  , -0.40541083]],      dtype=float32)}},\n",
      " 'linear2': {'bias': {'value': Array([0., 0., 0., 0.], dtype=float32)},\n",
      "             'kernel': {'value': Array([[ 0.21010089,  0.8289361 ,  0.04589564,  0.5422644 ],\n",
      "       [ 0.41914317,  0.84359694, -0.47937787, -0.49135214],\n",
      "       [-0.46072108,  0.4630125 ,  0.39276958, -0.9441406 ],\n",
      "       [-0.6690758 , -0.18474789, -0.57622856,  0.4821079 ]],      dtype=float32)}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivyzheng/envs/flax-head/lib/python3.11/site-packages/orbax/checkpoint/type_handlers.py:1439: UserWarning: Couldn't find sharding info under RestoreArgs. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file instead of directly from RestoreArgs. Note also that this option is unsafe when restoring on a different topology than the checkpoint was saved with.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def process_raw_dict(raw_state_dict):\n",
    "  flattened = nnx.traversals.flatten_mapping(raw_state_dict)\n",
    "  # Cut the '.value' postfix on every leaf path.\n",
    "  flattened = {(path[:-1] if path[-1] == 'value' else path): value\n",
    "               for path, value in flattened.items()}\n",
    "  return nnx.traversals.unflatten_mapping(flattened)\n",
    "\n",
    "# Make your local change on the checkpoint dictionary.\n",
    "raw_dict = checkpointer.restore('/tmp/nnx-surgery-state')\n",
    "pprint(raw_dict)\n",
    "raw_dict['layer1'] = raw_dict.pop('linear1')\n",
    "raw_dict['layer2'] = raw_dict.pop('linear2')\n",
    "\n",
    "# Fit it into the model state.\n",
    "abs_model = nnx.eval_shape(lambda: ModifiedTwoLayerMLP(4, rngs=nnx.Rngs(0)))\n",
    "graph_def, state = nnx.split(abs_model)\n",
    "nnx.replace_by_pure_dict(state, process_raw_dict(raw_dict))\n",
    "restored_model = nnx.merge(graph_def, state)\n",
    "\n",
    "np.testing.assert_allclose(restored_model(jnp.ones((3, 4))), old_model(jnp.ones((3, 4))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial initialization\n",
    "\n",
    "In some cases - such as with LoRA (Low-Rank Adaption) - you may want to randomly-initialize only *part of* your model parameters. This can be achieved through:\n",
    "\n",
    "- Naive partial initialization; or\n",
    "- Memory-efficient partial initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive partial initialization\n",
    "\n",
    "To do naive partial initialization, you can just initialize the whole model, then swap the pre-trained parameters in. However, this approach may allocate additional memory midway if your modification requires re-creating module parameters that you will later discard. Below is an example of this.\n",
    "\n",
    "> **Note:** You can use `jax.live_arrays()` to check all the arrays live in memory at any given time. This call can be “messed up” when you run a single Jupyter notebook cell multiple times (due to garbage-collection of old Python variables). However, restarting the Python kernel in the notebook and running the code from scratch will always yield the same output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jax arrays in memory at start: 38\n",
      "Number of jax arrays in memory midway: 42 (4 new created in LoRALinear - kernel, bias, lora_a & lora_b)\n",
      "Number of jax arrays in memory at end: 40 (2 discarded - only lora_a & lora_b are used in model)\n"
     ]
    }
   ],
   "source": [
    "# Some pretrained model state\n",
    "old_state = nnx.state(TwoLayerMLP(4, rngs=nnx.Rngs(0)))\n",
    "\n",
    "simple_model = nnx.eval_shape(lambda: TwoLayerMLP(4, rngs=nnx.Rngs(42)))\n",
    "print(f'Number of jax arrays in memory at start: {len(jax.live_arrays())}')\n",
    "# In this line, extra kernel and bias is created inside the new LoRALinear!\n",
    "# They are wasted, because you are going to use the kernel and bias in `old_state` anyway.\n",
    "simple_model.linear1 = nnx.LoRALinear(4, 4, lora_rank=3, rngs=nnx.Rngs(42))\n",
    "print(f'Number of jax arrays in memory midway: {len(jax.live_arrays())}'\n",
    "      ' (4 new created in LoRALinear - kernel, bias, lora_a & lora_b)')\n",
    "nnx.update(simple_model, old_state)\n",
    "print(f'Number of jax arrays in memory at end: {len(jax.live_arrays())}'\n",
    "      ' (2 discarded - only lora_a & lora_b are used in model)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory-efficient partial initialization\n",
    "\n",
    "To do memory-efficient partial initialization, use `nnx.jit`'s efficiently compiled code to make sure only the state parameters you need are initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jax arrays in memory at start: 44\n",
      "Number of jax arrays in memory at end: 46 (2 new created - lora_a and lora_b)\n"
     ]
    }
   ],
   "source": [
    "# Some pretrained model state\n",
    "old_state = nnx.state(TwoLayerMLP(4, rngs=nnx.Rngs(0)))\n",
    "\n",
    "# Use `nnx.jit` (which wraps `jax.jit`) to automatically skip unused arrays - memory efficient!\n",
    "@nnx.jit(donate_argnums=0)\n",
    "def partial_init(old_state, rngs):\n",
    "  model = TwoLayerMLP(4, rngs=rngs)\n",
    "  # Create a new state.\n",
    "  model.linear1 = nnx.LoRALinear(4, 4, lora_rank=3, rngs=rngs)\n",
    "  # Add the existing state.\n",
    "  nnx.update(model, old_state)\n",
    "  return model\n",
    "\n",
    "print(f'Number of JAX Arrays in memory at start: {len(jax.live_arrays())}')\n",
    "# Note that `old_state` will be deleted after this `partial_init` call.\n",
    "good_model = partial_init(old_state, nnx.Rngs(42))\n",
    "print(f'Number of JAX Arrays in memory at end: {len(jax.live_arrays())}'\n",
    "      ' (2 new created - lora_a and lora_b)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
