{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale up on multiple devices\n",
    "\n",
    "This guide demonstrates how to scale up a Flax NNX model on multiple accelerators (GPUs or Google TPUs) using JAX's parallel programming APIs.\n",
    "\n",
    "[Introduction to Parallel Programming](https://docs.jax.dev/en/latest/sharded-computation.html) is a fantastic guide to learn about the distributed programming essentials of JAX. It describes three parallelism APIs - automatic, explicit and manual - for different levels of control.\n",
    "\n",
    "This guide will primarily cover the automatic scenario, which use the [`jax.jit`](https://jax.readthedocs.io/en/latest/jit-compilation.html) to compile your single-device code as multi-device. You will use [`flax.nnx.spmd`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/spmd.html) APIs to annotate your model variables with how it should be sharded.\n",
    "\n",
    "If you want to follow explicit sharding style, follow [JAX Explicit Sharding](https://docs.jax.dev/en/latest/notebooks/explicit-sharding.html) guide and use JAX's relevant APIs. No API on Flax side is needed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 8 “fake” JAX devices now: [CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3), CpuDevice(id=4), CpuDevice(id=5), CpuDevice(id=6), CpuDevice(id=7)]\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, AxisType\n",
    "import optax\n",
    "import flax\n",
    "from flax import nnx\n",
    "\n",
    "# Ignore this if you are already running on a TPU or GPU\n",
    "if not jax._src.xla_bridge.backends_are_initialized():\n",
    "  jax.config.update('jax_num_cpu_devices', 8)\n",
    "print(f'You have 8 “fake” JAX devices now: {jax.devices()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a `2x4` device mesh as the [JAX data sharding tutorial](https://docs.jax.dev/en/latest/sharded-computation.html#key-concept-data-sharding) instructs.\n",
    "\n",
    "In this guide we use a standard FSDP layout and shard our devices on two axes - `data` and `model`, for doing batch data parallelism and tensor parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an auto-mode mesh of two dimensions and annotate each axis with a name.\n",
    "auto_mesh = jax.make_mesh((2, 4), ('data', 'model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compatibility Note: This guide covers the [eager sharding feature](https://flax.readthedocs.io/en/latest/flip/4844-var-eager-sharding.html) that greatly simplifies creating sharded model. If your project already used Flax GSPMD API on version `flax<0.12`, you might have turned the feature off to keep your code working. Users can toggle this feature using the `nnx.use_eager_sharding` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnx.use_eager_sharding(True)\n",
    "assert nnx.using_eager_sharding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24144d8",
   "metadata": {},
   "source": [
    "The `nnx.use_eager_sharding` function can also be used as a context manager to toggle the eager sharding feature within a specific scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d849e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with nnx.use_eager_sharding(False):\n",
    "  assert not nnx.using_eager_sharding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f808ec",
   "metadata": {},
   "source": [
    "You can also enable eager sharding on a per-variable basis by passing `eager_sharding=False` during variable initialization. The mesh can also be passed this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bbd440",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnx.Param(jnp.ones(4,4), sharding_names=(None, 'model'), eager_sharding=True, mesh=auto_mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shard a single-array model\n",
    "\n",
    "Let's begin by sharding the simplest component possible - a Flax variable.\n",
    "\n",
    "When you define a Flax variable, you can pass in a metadata field called `sharding_names`, to specify how the underlying JAX array should be sharded. This field should be a tuple of names, each of which refer to how an axis of the array should be sharded.\n",
    "\n",
    "**You must have an existing device mesh** and create a sharding-annotated `nnx.Variable` within its scope. This allows the result variable to be sharded accordingly on those devices. The device mesh can be your actual accelerator mesh, or a dummy fake CPU mesh like in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PartitionSpec(None, 'model')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  CPU 0,4   </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">  CPU 1,5   </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">  CPU 2,6   </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">  CPU 3,7   </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0,4\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mCPU 1,5\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m   \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m  \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74mCPU 2,6\u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m   \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m  \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107mCPU 3,7\u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m   \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rngs = nnx.Rngs(0)\n",
    "\n",
    "with jax.set_mesh(auto_mesh):\n",
    "  w = nnx.Param(\n",
    "    rngs.lecun_normal()((4, 8)),\n",
    "    sharding_names=(None, 'model')\n",
    "  )\n",
    "  print(w.sharding.spec)\n",
    "  jax.debug.visualize_array_sharding(w)  # already sharded!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize with style\n",
    "\n",
    "When using existing modules, you can apply [`flax.nnx.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/spmd.html#flax.nnx.with_partitioning) on initializers to achieve the same effect. Here we create a sharded `nnx.Linear` module with only the kernel weight.\n",
    "\n",
    "Also, you should use `jax.jit` for the whole initialization for maximum performance. This is because without `jax.jit`, a single-device variable must be created first before we apply sharding constraints and then make it sharded, which is wasteful. `jax.jit` will automatically optimize this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def init_sharded_linear(key):\n",
    "  init_fn = nnx.nn.linear.default_kernel_init\n",
    "  # Shard your parameter along `model` dimension, as in model/tensor parallelism\n",
    "  return nnx.Linear(4, 8, use_bias=False, rngs=nnx.Rngs(key),\n",
    "                    kernel_init=nnx.with_partitioning(init_fn, (None, 'model')))\n",
    "\n",
    "with jax.set_mesh(auto_mesh):\n",
    "  key= rngs()\n",
    "  linear = init_sharded_linear(key)\n",
    "  assert linear.kernel.sharding.spec == P(None, 'model') # already sharded!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model\n",
    "\n",
    "If you also shard your input correctly, JAX would be able to carry out the most natural and optimized computation and produce your output as sharded.\n",
    "\n",
    "You should still make sure to `jax.jit` for maximum performance, and also to explicitly control how each array is sharded when you want to. We will give an example of that control in the next section.\n",
    "\n",
    "> Note: You need to `jax.jit` a pure function that takes the model as an argument, instead of jitting the callable model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PartitionSpec('data', 'model')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  CPU 0  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">  CPU 1  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">  CPU 2  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">  CPU 3  </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">         </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">         </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">  CPU 4  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">  CPU 5  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">  CPU 6  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">  CPU 7  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">         </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">         </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">         </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m         \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m         \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m  \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107mCPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m  \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m  \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82mCPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m  \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mCPU 3\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m         \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m         \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m         \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;231;203;148m         \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m         \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m         \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m         \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;231;203;148m         \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m         \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m         \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m         \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;231;203;148m  \u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148mCPU 4\u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148m  \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m  \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207mCPU 5\u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m  \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m  \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148mCPU 6\u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m  \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m  \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49mCPU 7\u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m  \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;231;203;148m         \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m         \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m         \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m         \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;231;203;148m         \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m         \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m         \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m         \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;231;203;148m         \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m         \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m         \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m         \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For simple computations, you can get correctly-sharded output without jitting\n",
    "# In this case, ('data', None) @ (None, 'model') = ('data', 'model')\n",
    "with jax.set_mesh(auto_mesh):\n",
    "  # Create your input data, sharded along `data` dimension, as in data parallelism\n",
    "  x = jax.device_put(jnp.ones((16, 4)), P('data', None))\n",
    "\n",
    "  # Run the model forward function, jitted\n",
    "  y = jax.jit(lambda m, x: m(x))(linear, x)\n",
    "  print(y.sharding.spec)                       # sharded: ('data', 'model')\n",
    "  jax.debug.visualize_array_sharding(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shard a wholesome model\n",
    "\n",
    "Now we construct a more wholesome model to show a few advanced tricks. Check out this simple `DotReluDot` module that does two matmuls, and the `MultiDotReluDot` module that creates an arbitrary stack of `DotReluDot` sublayers.\n",
    "\n",
    "Make note of the following:\n",
    "\n",
    "* **Additional axis annotation**: Transforms like `vmap` and `scan` will add additional dimensions to the JAX arrays. Unfortunately, in auto sharding mode you will need to use `nnx.vmap` and `nnx.scan` instead of raw JAX transforms, so that both JAX and Flax knows how to shard this dimension. You won't need this in [explicit sharding mode](#explicit-sharding).\n",
    "\n",
    "* [`jax.lax.with_sharding_constraint`](https://docs.jax.dev/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html#constraining-shardings-of-intermediates-in-jitted-code): They can help you to enforce specific shardings on intermediate activations. Only works under an auto mode mesh context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotReluDot(nnx.Module):\n",
    "  def __init__(self, depth: int, rngs: nnx.Rngs):\n",
    "    init_fn = nnx.initializers.lecun_normal()\n",
    "    self.dot1 = nnx.Linear(\n",
    "      depth, depth,\n",
    "      kernel_init=nnx.with_partitioning(init_fn, (None, 'model')),\n",
    "      use_bias=False,  # or use `bias_init` to give it annotation too\n",
    "      rngs=rngs)\n",
    "    self.w2 = nnx.Param(\n",
    "      init_fn(rngs.params(), (depth, depth)),  # RNG key and shape for W2 creation\n",
    "      sharding=('model', None),\n",
    "    )\n",
    "\n",
    "  def __call__(self, x: jax.Array):\n",
    "    y = self.dot1(x)\n",
    "    y = jax.nn.relu(y)\n",
    "    y = jax.lax.with_sharding_constraint(y, P('data', 'model'))\n",
    "    z = jnp.dot(y, self.w2[...])\n",
    "    return z\n",
    "\n",
    "class MultiDotReluDot(nnx.Module):\n",
    "  def __init__(self, depth: int, num_layers: int, rngs: nnx.Rngs):\n",
    "    # Annotate the additional axis with sharding=None, meaning it will be\n",
    "    # replicated across all devices.\n",
    "    @nnx.vmap(transform_metadata={nnx.PARTITION_NAME: None})\n",
    "    def create_sublayers(r):\n",
    "      return DotReluDot(depth, r)\n",
    "    self.layers = create_sublayers(rngs.fork(split=num_layers))\n",
    "\n",
    "  def __call__(self, x):\n",
    "    def scan_over_layers(x, layer):\n",
    "      return layer(x), None\n",
    "    x, _ = jax.lax.scan(scan_over_layers, x, self.layers)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a sample training loop, using `jax.jit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.251457\n",
      "0.8495563\n",
      "0.6590716\n",
      "0.5399748\n",
      "0.39150265\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def train_step(model, optimizer, x, y):\n",
    "  def loss_fn(model: DotReluDot):\n",
    "    y_pred = model(x)\n",
    "    return jnp.mean((y_pred - y) ** 2)\n",
    "\n",
    "  loss, grads = jax.value_and_grad(loss_fn)(model)\n",
    "  optimizer.update(model, grads)\n",
    "  return model, loss\n",
    "\n",
    "\n",
    "with jax.set_mesh(auto_mesh):\n",
    "  # Training data\n",
    "  input = jax.device_put(rngs.normal((8, 1024)), P('data', None))\n",
    "  label = jax.device_put(rngs.normal((8, 1024)), P('data', None))\n",
    "  # Model and optimizer\n",
    "  model = MultiDotReluDot(1024, 2, rngs=nnx.Rngs(0))\n",
    "  optimizer = nnx.Optimizer(model, optax.adam(1e-3), wrt=nnx.Param)\n",
    "\n",
    "  # The loop\n",
    "  for i in range(5):\n",
    "    model, loss = train_step(model, optimizer, input, label)\n",
    "    print(loss)    # Model (over-)fitting to the labels quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling\n",
    "\n",
    "If you are using a Google TPU pod or a pod slice, you can create a custom `block_all()` utility function, as defined below, to measure the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 ms ± 588 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "def block_all(xs):\n",
    "  jax.tree_util.tree_map(lambda x: x.block_until_ready(), xs)\n",
    "  return xs\n",
    "\n",
    "with jax.set_mesh(auto_mesh):\n",
    "  new_state = block_all(train_step(model, optimizer, input, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a sharded model from a checkpoint\n",
    "\n",
    "Now you learned how to initialize a sharded model without OOM, but what about saving and loading it from a checkpoint on disk? JAX checkpointing libraries, such as [Orbax](https://orbax.readthedocs.io/en/latest/), support loading a model distributedly if a sharding pytree is provided. Below is an example that uses Orbax's `StandardCheckpointer` API.\n",
    "\n",
    "Make sure you save a model's state, especially if your model shares some variables across modules. Given a You can generate an identical abstract pytree with shardings using Flax’s `nnx.get_abstract_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PartitionSpec(None, None, 'model')\n",
      "(2, 1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "import orbax.checkpoint as ocp\n",
    "\n",
    "# Save the sharded state.\n",
    "sharded_state = nnx.state(model)\n",
    "path = ocp.test_utils.erase_and_create_empty('/tmp/my-checkpoints/')\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "checkpointer.save(path / 'checkpoint_name', sharded_state)\n",
    "\n",
    "# Load a sharded state from the checkpoint.\n",
    "graphdef, abs_state = nnx.get_abstract_model(\n",
    "  lambda: MultiDotReluDot(1024, 2, rngs=nnx.Rngs(0)), auto_mesh)\n",
    "restored_state = checkpointer.restore(path / 'checkpoint_name',\n",
    "                                      target=abs_state)\n",
    "restored_model = nnx.merge(graphdef, abs_state)\n",
    "print(restored_model.layers.dot1.kernel.sharding.spec)\n",
    "print(restored_model.layers.dot1.kernel.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logical axis annotation\n",
    "\n",
    "JAX's [automatic](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html) [SPMD](https://jax.readthedocs.io/en/latest/glossary.html#term-SPMD) encourages users to explore different sharding layouts to find the optimal one. To this end, in Flax you have the option to annotate with more descriptive axis names (not just device mesh axis names like `'data'` and `'model'`), as long as you provide a mapping from your alias to the device mesh axes.\n",
    "\n",
    "You can provide the mapping along with the annotation as another metadata of the corresponding [`nnx.Variable`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/variables.html#flax.nnx.Variable), or overwrite it at top-level. Check out the `LogicalDotReluDot` example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mapping from alias annotation to the device mesh.\n",
    "sharding_rules = (('batch', 'data'), ('hidden', 'model'), ('embed', None))\n",
    "\n",
    "class LogicalDotReluDot(nnx.Module):\n",
    "  def __init__(self, depth: int, rngs: nnx.Rngs):\n",
    "    init_fn = nnx.initializers.lecun_normal()\n",
    "    self.dot1 = nnx.Linear(\n",
    "      depth, depth,\n",
    "      kernel_init=nnx.with_partitioning(init_fn, ('embed', 'hidden')),\n",
    "      use_bias=False,  # or use `bias_init` to give it annotation too\n",
    "      rngs=rngs)\n",
    "    self.w2 = nnx.Param(\n",
    "      init_fn(rngs.params(), (depth, depth)),  # RNG key and shape for W2 creation\n",
    "      sharding=('hidden', 'embed'),\n",
    "    )\n",
    "\n",
    "  def __call__(self, x: jax.Array):\n",
    "    y = self.dot1(x)\n",
    "    y = jax.nn.relu(y)\n",
    "    # Unfortunately the logical aliasing doesn't work on lower-level JAX calls.\n",
    "    y = jax.lax.with_sharding_constraint(y, P('data', None))\n",
    "    z = jnp.dot(y, self.w2[...])\n",
    "    return z\n",
    "\n",
    "class LogicalMultiDotReluDot(nnx.Module):\n",
    "  def __init__(self, depth: int, num_layers: int, rngs: nnx.Rngs):\n",
    "    @nnx.vmap(transform_metadata={nnx.PARTITION_NAME: None})\n",
    "    def create_sublayers(r):\n",
    "      return LogicalDotReluDot(depth, r)\n",
    "    self.layers = create_sublayers(rngs.fork(split=num_layers))\n",
    "\n",
    "  def __call__(self, x):\n",
    "    def scan_over_layers(x, layer):\n",
    "      return layer(x), None\n",
    "    x, _ = jax.lax.scan(scan_over_layers, x, self.layers)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you didn't provide all `sharding_rule` annotations in the model definition, you can apply them at top level by put them into the context via `nnx.logical_axis_rules`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jax.set_mesh(auto_mesh), nnx.logical_axis_rules(sharding_rules):\n",
    "  # Model and optimizer\n",
    "  logical_model = LogicalMultiDotReluDot(1024, 2, rngs=nnx.Rngs(0))\n",
    "  logical_output = logical_model(input)\n",
    "\n",
    "# Check out their equivalency with some easier-to-read sharding descriptions.\n",
    "assert logical_model.layers.dot1.kernel.sharding.is_equivalent_to(\n",
    "  NamedSharding(auto_mesh, P(None, None, 'model')), ndim=3\n",
    ")\n",
    "assert logical_model.layers.w2.sharding.is_equivalent_to(\n",
    "  NamedSharding(auto_mesh, P(None, 'model', None)), ndim=3\n",
    ")\n",
    "assert logical_output.sharding.is_equivalent_to(\n",
    "  NamedSharding(auto_mesh, P('data', None)), ndim=2\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use device axis / logical axis\n",
    "\n",
    "Choosing when to use a device or logical axis depends on how much you want to control the partitioning of your model:\n",
    "\n",
    "* **Device mesh axis**:\n",
    "\n",
    "  * For a simpler model, this can save you a few extra lines of code of converting the logical naming back to the device naming.\n",
    "\n",
    "  * Shardings of intermediate *activation* values can only be done via [`jax.lax.with_sharding_constraint`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.with_sharding_constraint.html) and device mesh axis. Therefore, if you want super fine-grained control over your model's sharding, directly using device mesh axis names everywhere might be less confusing.\n",
    "\n",
    "* **Logical naming**: This is helpful if you want to experiment around and find the most optimal partition layout for your *model weights*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicit sharding\n",
    "\n",
    "[Explicit sharding](https://docs.jax.dev/en/latest/notebooks/explicit-sharding.html), also called \"sharding-in-types\", is a new JAX sharding feature that allows every sharding of every array to be deterministic and explicit. Instead of letting XLA compiler figure out the shardings, you as user would explicitly state the shardings via JAX APIs.\n",
    "\n",
    "For education purposes, we provide a simple Flax model example using explicit sharding. Note how you specify shardings for this model:\n",
    "\n",
    "* Parameters: `out_sharding` argument passed into JAX initializers.\n",
    "\n",
    "* Ambigious computations like `jnp.dot`: provide `out_sharding` argument to specify the output sharding.\n",
    "\n",
    "* Additional dimension from transforms: use `jax.vmap`'s argument `spmd_axis_name`, instead of Flax lifted transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PartitionSpec(None, None, 'model')\n",
      "PartitionSpec(None, 'model', None)\n"
     ]
    }
   ],
   "source": [
    "# Explicit axis mesh\n",
    "explicit_mesh = jax.make_mesh((2, 4), ('data', 'model'),\n",
    "                              axis_types=(AxisType.Explicit, AxisType.Explicit))\n",
    "\n",
    "class ExplicitDotReluDot(nnx.Module):\n",
    "  def __init__(self, depth: int, rngs: nnx.Rngs):\n",
    "    init_fn = nnx.initializers.lecun_normal()\n",
    "    self.dot1 = nnx.Linear(\n",
    "      depth, depth,\n",
    "      kernel_init=partial(init_fn, out_sharding=P(None, 'model')),\n",
    "      use_bias=False,\n",
    "      rngs=rngs)\n",
    "    self.w2 = nnx.Param(\n",
    "      init_fn(rngs.params(), (depth, depth), out_sharding=P('model', None)),\n",
    "    )\n",
    "    self.b2 = nnx.Param(jnp.zeros((depth, ), out_sharding=P(None,)))\n",
    "\n",
    "  def __call__(self, x: jax.Array):\n",
    "    y = self.dot1(x)\n",
    "    y = jax.nn.relu(y)\n",
    "    z = jnp.dot(y, self.w2[...], out_sharding=P('data', None))\n",
    "    z = z + self.b2\n",
    "    return z\n",
    "\n",
    "\n",
    "class ExplicitMultiDotReluDot(nnx.Module):\n",
    "  def __init__(self, depth: int, num_layers: int, rngs: nnx.Rngs):\n",
    "    # Annotate the additional axis with sharding=None, meaning it will be\n",
    "    # replicated across all devices.\n",
    "    @partial(jax.vmap, spmd_axis_name=None)\n",
    "    def create_sublayers(r):\n",
    "      return ExplicitDotReluDot(depth, r)\n",
    "    self.layers = create_sublayers(rngs.fork(split=num_layers))\n",
    "\n",
    "  def __call__(self, x):\n",
    "    def scan_over_layers(x, layer):\n",
    "      return layer(x), None\n",
    "    x, _ = jax.lax.scan(scan_over_layers, x, self.layers)\n",
    "    return x\n",
    "\n",
    "\n",
    "with jax.set_mesh(explicit_mesh):\n",
    "  model = ExplicitMultiDotReluDot(1024, 2, rngs=nnx.Rngs(0))\n",
    "  x = jax.device_put(rngs.normal((8, 1024)),\n",
    "                     NamedSharding(explicit_mesh, P('data', None)))\n",
    "  y = model(x)\n",
    "\n",
    "print(model.layers.dot1.kernel.sharding.spec)\n",
    "print(model.layers.w2.sharding.spec)\n",
    "assert x.sharding.is_equivalent_to(y.sharding, ndim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing easier in explicit mode is that you can obtain the abstract array tree with shardings via `jax.eval_shape`, instead of calling `nnx.get_abstract_sharding`. This is not possible in auto mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PartitionSpec(None, None, 'model')\n",
      "PartitionSpec(None, 'model', None)\n"
     ]
    }
   ],
   "source": [
    "# Get the sharding tree to load checkpoint with\n",
    "with jax.set_mesh(explicit_mesh):\n",
    "  abs_model = jax.eval_shape(\n",
    "    lambda: ExplicitMultiDotReluDot(1024, 2, rngs=nnx.Rngs(0)))\n",
    "  print(abs_model.layers.dot1.kernel.sharding.spec)\n",
    "  print(abs_model.layers.w2.sharding.spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further readings\n",
    "\n",
    "JAX has abundant documentation on scaled computing.\n",
    "\n",
    "- [Introduction to parallel programming](https://jax.readthedocs.io/en/latest/sharded-computation.html): A 101 level tutorial covering the basics of automatic parallelization with [`jax.jit`](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html#jax.jit), semi-automatic parallelization with [`jax.jit`](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html) and [`jax.lax.with_sharding_constraint`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.with_sharding_constraint.html), and manual sharding with [`shard_map`](https://jax.readthedocs.io/en/latest/_autosummary/jax.experimental.shard_map.shard_map.html#jax.experimental.shard_map.shard_map).\n",
    "- [JAX in multi-process environments](https://jax.readthedocs.io/en/latest/multi_process.html).\n",
    "- [Distributed arrays and automatic parallelization](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html): A more detailed tutorial about parallelization with [`jax.jit`](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html#jax.jit) and [`jax.lax.with_sharding_constraint`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.with_sharding_constraint.html)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
